{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Starting on the Advanced Analytics Workspace \u00b6 The Advanced Analytics Workspace portal is a great place to discover and connect to the available resources we'll be talking about here. We'll break down the standard tasks into three categories: Experimentation / Analysis Publishing Large scale production All are important, and we will address all of them, but we'll focus on the first two as these are most widely applicable. For Experiments \u00b6 Jupyter notebooks \u00b6 R , Python , and Julia Choose the CPU/RAM you need, big or small, to fit your analysis Share your workspace with your team, along with the data and notebooks within Learn More Desktops with ML-Workspace \u00b6 Notebooks are more easily shared than desktops, but we also have the ability to run a full desktop, with typical applications, right inside your browser. Learn More For Publishing \u00b6 R Shiny \u00b6 The platform is designed to host any kind of open source application you want. We have an R-Shiny server for hosting R-Shiny apps To create any an R-Shiny Dashboard, you just have to submit a GitHub pull request to our R-Dashboards GitHub repository . For Production \u00b6 If an experiment turns into a product, then one of the following may be needed: Kubeflow pipelines for high-volume/intensity work Automation pipelines Ask for help in production The Advanced Analytics Workspace support staff are happy to help with production oriented use cases, and we can probably save you lots of time. Don't be shy to ask us for help ! How do I get data? How do I submit data? \u00b6 Every workspace can be equipped with its own storage. There are also storage buckets to publish datasets; either for internal use or for wider release. We will give an overview of the technologies here, and in the next sections there will be a more in-depth description of each of them. Browse some datasets Browse some datasets here. These data sets are meant to store widely shared data. Either data that has been brought it, or data to be released out as a product. As always, ensure that the data is not sensitive.","title":"Getting Started"},{"location":"#starting-on-the-advanced-analytics-workspace","text":"The Advanced Analytics Workspace portal is a great place to discover and connect to the available resources we'll be talking about here. We'll break down the standard tasks into three categories: Experimentation / Analysis Publishing Large scale production All are important, and we will address all of them, but we'll focus on the first two as these are most widely applicable.","title":"Starting on the Advanced Analytics Workspace"},{"location":"#for-experiments","text":"","title":"For Experiments"},{"location":"#jupyter-notebooks","text":"R , Python , and Julia Choose the CPU/RAM you need, big or small, to fit your analysis Share your workspace with your team, along with the data and notebooks within Learn More","title":"Jupyter notebooks"},{"location":"#desktops-with-ml-workspace","text":"Notebooks are more easily shared than desktops, but we also have the ability to run a full desktop, with typical applications, right inside your browser. Learn More","title":"Desktops with ML-Workspace"},{"location":"#for-publishing","text":"","title":"For Publishing"},{"location":"#r-shiny","text":"The platform is designed to host any kind of open source application you want. We have an R-Shiny server for hosting R-Shiny apps To create any an R-Shiny Dashboard, you just have to submit a GitHub pull request to our R-Dashboards GitHub repository .","title":"R Shiny"},{"location":"#for-production","text":"If an experiment turns into a product, then one of the following may be needed: Kubeflow pipelines for high-volume/intensity work Automation pipelines Ask for help in production The Advanced Analytics Workspace support staff are happy to help with production oriented use cases, and we can probably save you lots of time. Don't be shy to ask us for help !","title":"For Production"},{"location":"#how-do-i-get-data-how-do-i-submit-data","text":"Every workspace can be equipped with its own storage. There are also storage buckets to publish datasets; either for internal use or for wider release. We will give an overview of the technologies here, and in the next sections there will be a more in-depth description of each of them. Browse some datasets Browse some datasets here. These data sets are meant to store widely shared data. Either data that has been brought it, or data to be released out as a product. As always, ensure that the data is not sensitive.","title":"How do I get data? How do I submit data?"},{"location":"Collaboration/","text":"Collaboration on the Advanced Analytics Workspace \u00b6 There are lots of ways to collaborate on the platform, and what's best for you depends on what you're sharing and how many people you want to share with . We can roughly break the shareable things into Data and Code , and we can share the scope of who you're sharing with No one vs. My Team vs. Everyone . This leads to the following table of options Private Team StatCan Code GitLab/GitHub or personal folder GitLab/GitHub or team folder GitLab/GitHub Data Personal folder or bucket Team folder or bucket Shared Bucket What is the difference between a bucket and a folder? Buckets are like Network Storage. See the Storage section section for more discussion of the differences between these two ideas. The way that Private vs. Team based access is configured is with namespaces . So we start by talking about Kubeflow and Kubeflow namespaces. Then, Data and Code are better handled with slightly different tools, so we discuss the two separately. With Data we discuss buckets and MinIO , and with Code we discuss git , GitLab , and GitHub . This motivates the structure of this page Team Based Collaboration (applicable to Code and Data) Sharing Code Sharing Data Team Based Collaboration \u00b6 What does Kubeflow do? \u00b6 Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and your team can work along side you. Requesting a namespace \u00b6 By default, everyone gets their own personal namespace, firstname-lastname . But if you want to create a namespace for a team, then there is a button to submit a request for a namespace on the portal. Click the \u22ee menu on the Kubeflow section of the portal . The namespace cannot have special characters other than hyphens The namespace name must only be lowercase letters a-z with dashes. Otherwise, the namespace will not be created. You will receive an email notification when the namespace is created. Once the shared namespace is created, you can access it the same as any other namespace you have through the Kubeflow UI, like shown below. You will then be able to manage the collaborators list through Kubeflow's Manage Contributors tab, where you can add your colleagues to the shared namespace. To switch namespaces, take a look at the top of your window, just to the right of the Kubeflow Logo. Shared Code \u00b6 Teams have two options (but you can combine both!): Share a workspace in Kubeflow \u00b6 The advantage of sharing inside Kubeflow is that it's more free-form and it works better for .ipynb files (Jupyter notebooks). This method also lets you share a compute environment, so you can share resources very easily. When you share a workspace, you share A Private and Shared bucket ( /team-name and /shared/team-name ) All notebook servers in the Kubeflow Namespace Share with git, using GitLab or GitHub \u00b6 The advantage of sharing with git is that it works with users across namespaces, and keeping code in git is a great way to manage large software projects. Don't forget to include a License! If your code is public, do not forget to keep with the Innovation Team's guidelines and use a proper License if your work is done for Statistics Canada. Recommendation: Combine both \u00b6 It's a great idea to always use git, and using git along with shared workspaces is a great way to combine ad hoc sharing (through files) while also keeping your code organized and tracked. Shared Storage \u00b6 Sharing with your team \u00b6 Once you have a shared namespace, you have two shared storage approaches Storage Option Benefits Shared Jupyter Servers/Workspaces More amenable to small files, notebooks, and little experiments. Shared Buckets (see Storage ) Better suited for use in pipelines, APIs, and for large files. To learn more about the technology behind these, check out the Storage section . Sharing with StatCan \u00b6 In addition to private buckets, or team-shared private buckets, you can also place your files in shared storage . Within all bucket storage options ( minimal , premium , pachyderm ), you have a private bucket, and a folder inside of the shared bucket. Take a look, for instance, at the link below: shared/blair-drummond/ Any logged in user can see these files and read them freely. Sharing with the world \u00b6 Ask about that one in our Slack channel . There are many ways to do this from the IT side, but it's important for it to go through proper processes, so this is not done in a \"self-serve\" way that the others are. That said, it is totally possible.","title":"Collaboration"},{"location":"Collaboration/#collaboration-on-the-advanced-analytics-workspace","text":"There are lots of ways to collaborate on the platform, and what's best for you depends on what you're sharing and how many people you want to share with . We can roughly break the shareable things into Data and Code , and we can share the scope of who you're sharing with No one vs. My Team vs. Everyone . This leads to the following table of options Private Team StatCan Code GitLab/GitHub or personal folder GitLab/GitHub or team folder GitLab/GitHub Data Personal folder or bucket Team folder or bucket Shared Bucket What is the difference between a bucket and a folder? Buckets are like Network Storage. See the Storage section section for more discussion of the differences between these two ideas. The way that Private vs. Team based access is configured is with namespaces . So we start by talking about Kubeflow and Kubeflow namespaces. Then, Data and Code are better handled with slightly different tools, so we discuss the two separately. With Data we discuss buckets and MinIO , and with Code we discuss git , GitLab , and GitHub . This motivates the structure of this page Team Based Collaboration (applicable to Code and Data) Sharing Code Sharing Data","title":"Collaboration on the Advanced Analytics Workspace"},{"location":"Collaboration/#team-based-collaboration","text":"","title":"Team Based Collaboration"},{"location":"Collaboration/#what-does-kubeflow-do","text":"Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and your team can work along side you.","title":"What does Kubeflow do?"},{"location":"Collaboration/#requesting-a-namespace","text":"By default, everyone gets their own personal namespace, firstname-lastname . But if you want to create a namespace for a team, then there is a button to submit a request for a namespace on the portal. Click the \u22ee menu on the Kubeflow section of the portal . The namespace cannot have special characters other than hyphens The namespace name must only be lowercase letters a-z with dashes. Otherwise, the namespace will not be created. You will receive an email notification when the namespace is created. Once the shared namespace is created, you can access it the same as any other namespace you have through the Kubeflow UI, like shown below. You will then be able to manage the collaborators list through Kubeflow's Manage Contributors tab, where you can add your colleagues to the shared namespace. To switch namespaces, take a look at the top of your window, just to the right of the Kubeflow Logo.","title":"Requesting a namespace"},{"location":"Collaboration/#shared-code","text":"Teams have two options (but you can combine both!):","title":"Shared Code"},{"location":"Collaboration/#share-a-workspace-in-kubeflow","text":"The advantage of sharing inside Kubeflow is that it's more free-form and it works better for .ipynb files (Jupyter notebooks). This method also lets you share a compute environment, so you can share resources very easily. When you share a workspace, you share A Private and Shared bucket ( /team-name and /shared/team-name ) All notebook servers in the Kubeflow Namespace","title":"Share a workspace in Kubeflow"},{"location":"Collaboration/#share-with-git-using-gitlab-or-github","text":"The advantage of sharing with git is that it works with users across namespaces, and keeping code in git is a great way to manage large software projects. Don't forget to include a License! If your code is public, do not forget to keep with the Innovation Team's guidelines and use a proper License if your work is done for Statistics Canada.","title":"Share with git, using GitLab or GitHub"},{"location":"Collaboration/#recommendation-combine-both","text":"It's a great idea to always use git, and using git along with shared workspaces is a great way to combine ad hoc sharing (through files) while also keeping your code organized and tracked.","title":"Recommendation: Combine both"},{"location":"Collaboration/#shared-storage","text":"","title":"Shared Storage"},{"location":"Collaboration/#sharing-with-your-team","text":"Once you have a shared namespace, you have two shared storage approaches Storage Option Benefits Shared Jupyter Servers/Workspaces More amenable to small files, notebooks, and little experiments. Shared Buckets (see Storage ) Better suited for use in pipelines, APIs, and for large files. To learn more about the technology behind these, check out the Storage section .","title":"Sharing with your team"},{"location":"Collaboration/#sharing-with-statcan","text":"In addition to private buckets, or team-shared private buckets, you can also place your files in shared storage . Within all bucket storage options ( minimal , premium , pachyderm ), you have a private bucket, and a folder inside of the shared bucket. Take a look, for instance, at the link below: shared/blair-drummond/ Any logged in user can see these files and read them freely.","title":"Sharing with StatCan"},{"location":"Collaboration/#sharing-with-the-world","text":"Ask about that one in our Slack channel . There are many ways to do this from the IT side, but it's important for it to go through proper processes, so this is not done in a \"self-serve\" way that the others are. That said, it is totally possible.","title":"Sharing with the world"},{"location":"Help/","text":"Have questions? Or feedback? \u00b6 Come join us on the Advanced Analytics Workspace Slack channel ! You will find a bunch of other users of the platform there who may be able to answer your questions, and some of the engineers will usually be present on the channel. You can ask questions and provide feedback there. Slack (en) We will also post notices there if there are updates or downtime. Video tutorials \u00b6 After you have joined our Slack community, go and check out the following tutorials: Platform official Community driven content GitHub \u00b6 Want to know even more about our platform? Find everything about it on our GitHub page. COVID-19 Advanced Analytics Workspace on GitHub","title":"Help/Contact"},{"location":"Help/#have-questions-or-feedback","text":"Come join us on the Advanced Analytics Workspace Slack channel ! You will find a bunch of other users of the platform there who may be able to answer your questions, and some of the engineers will usually be present on the channel. You can ask questions and provide feedback there. Slack (en) We will also post notices there if there are updates or downtime.","title":"Have questions? Or feedback?"},{"location":"Help/#video-tutorials","text":"After you have joined our Slack community, go and check out the following tutorials: Platform official Community driven content","title":"Video tutorials"},{"location":"Help/#github","text":"Want to know even more about our platform? Find everything about it on our GitHub page. COVID-19 Advanced Analytics Workspace on GitHub","title":"GitHub"},{"location":"Storage/","text":"Overview \u00b6 The platform provides several types of storage: Disk (also called Volumes on the Notebook Server creation screen) Bucket (\"Blob\" or S3 storage, provided through MinIO) Data Lakes (coming soon) Depending on your use case, either disk or bucket may be most suitable: Type Simultaneous Users Speed Total size Sharable with Other Users Disk One machine/notebook server at a time Fastest (throughput and latency) <=512GB total per drive No Bucket Simultaneous access from many machines/notebook servers at the same time Fast-ish (Fast download, modest upload, modest latency) Infinite (within reason) Yes If you're unsure which to choose, don't sweat it These are guidelines, not an exact science - pick what sounds best now and run with it. The best choice for a complicated usage is non-obvious and often takes hands-on experience, so just trying something will help. For most situations both options work well even if they're not perfect, and remember that data can always be copied later if you change your mind. Disks \u00b6 Overview \u00b6 Disks are the familiar hard drive style file systems you're used to, provided to you from fast solid state drives (SSDs)! When creating your notebook server, you request them by adding Data Volumes to your notebook server (pictured below, with Type = New ). They are automatically mounted at the directory ( Mount Point ) you choose, and serve as a simple and reliable way to preserve data attached to a Notebook Server. When you delete your Notebook Server, your disks are not deleted . This let's you reuse that same disk (with all its contents) on a new Notebook Server later (as shown above with Type = Existing and the Name set to the volume you want to reuse). If you're done with the disk and it's contents, delete it . You pay for all disks you own, whether they're attached to a Notebook Server or not As soon as you create a disk, you're paying for it until it is deleted , even if it's original Notebook Server is deleted. See Deleting Disk Storage for more info Pricing \u00b6 Pricing models are tentative and may change As of writing, pricing is covered by the platform for initial users. This guidance explains how things are expected to be priced priced in future, but this may change. When mounting a disk, you get an Azure Managed Disk . The Premium SSD Managed Disks pricing shows the cost per disk based on size. Note that you pay for the size of disk requested, not the amount of space you are currently using. Tips to minimize costs As disks can be attached to a Notebook Server and reused, a typical usage pattern could be: At 9AM, create a Notebook Server (request 2CPU/8GB RAM and a 32GB attached disk) Do work throughout the day, saving results to the attached disk At 5PM, shut down your Notebook Server to avoid paying for it overnight NOTE: The attached disk is not destroyed by this action At 9AM the next day, create a new Notebook Server and attach your existing disk Continue your work... This keeps all your work safe without paying for the computer when you're not using it Deleting Disk Storage \u00b6 To see your disks, check the Notebook Volumes section of the Notebook Server page (shown below). You can delete any unattached disk (orange icon on the left) by clicking the trash can icon. Buckets (via MinIO) \u00b6 (2021-02-12: To improve stability and security, we have implemented new MinIO instances to replace the originals. We strongly suggest you use the new instances as soon as possible See Bucket Types and Access Scopes ) for more details Buckets are slightly more complicated, but they are good at three things: Large amounts of data Buckets can be huge: way bigger than hard drives. And they are still fast. Accessible by multiple consumers at once You can access the same data source from multiple Notebook Servers and pipelines at the same time without needing to duplicate the data Sharing You can share files from a bucket by sharing a URL that you can get through a simple web interface. This is great for sharing data with people outside of your workspace. There are several implementations of Accessing your Bucket \u00b6 There are multiple ways to upload and download data from your MinIO buckets: Mounted folders on a Notebook Server MinIO web portal MinIO command line tool mc Other S3-Compliant Methods Different access methods have strengths and weaknesses, but the data all goes to the same place You can: Upload a file using the mounted folder on a notebook server Rename that file using the web portal Download that file using the mc tool and all steps will be working on the same file . This lets you mix and match based on what is easiest for your tasks. There are two different MinIO services The examples below use the standard-tenant-1 instance of MinIO, but there is also a second instance: premium-tenant-1 . See Bucket Types and Access Scopes for more details. To use premium-tenant-1 in these examples, just substitute that name in for standard-tenant-1 . MinIO Mounted Folders on a Notebook Server \u00b6 Automatically, all Notebook Servers have your MinIO storage mounted as directories. This is located in ~/minio : These folders can be used like any other - you can copy files to/from using the file browser, write from Python/R, etc. The only difference is that the data is being stored in the MinIO service rather than on a local disk (and is thus accessible wherever you can access your MinIO bucket, rather than just from the Notebook Server it is attached to like a Disk ). Files copied into a mounted MinIO folder might take a few moments to be readable When you copy files into a MinIO folder, they are immediately stored and accessible in MinIO (e.g.: you can immediately see them in the Web Portal ). But, new files may take a few moments for the mounting service to notice and serve them in the mounted folder. If your use case needs access to these files immediately after copying them, try the other read methods ( the mc tool or Other S3 Compliant Methods ). MinIO Web Portal \u00b6 The MinIO service can be accessible through a web portal . To sign in using your existing credentials, use the \"Log in with OpenID\" button. From this portal you can browse to your personal bucket, which has the same name as your Kubeflow namespace (likely firstname-lastname ): This lets you browse, upload/download, delete, or share files. MinIO Command Line Tool \u00b6 MinIO provides the command line tool mc to access your data from a terminal. mc can communicate with one or more MinIO instances to let you upload/download files. For example: To run the below example yourself, replace BUCKETNAME 's value with your first/last name. For example: BUCKETNAME=john-smith . #!/bin/sh # The name of your bucket. This MUST be the same as your namespace's name. # Typically this is \"firstname-lastname\", but it might be different if working in a shared namespace BUCKETNAME = firstname-lastname # Get your personal credentials for the \"minio-standard-tenant-1\" MinIO instance # (this initializes $MINIO_URL, $MINIO_ACCESS_KEY, and $MINIO_SECRET_KEY environment variables) source /vault/secrets/minio-standard-tenant-1 # Create a MinIO alias (called \"standard\") for \"standard\" using your credentials mc config host add standard $MINIO_URL $MINIO_ACCESS_KEY $MINIO_SECRET_KEY # Create a bucket under your name # NOTE: You can *only* create buckets named the same as your namespace. Any # other name will be rejected. # Private bucket (\"mb\" = \"make bucket\") mc mb -p standard/ ${ BUCKETNAME } # Shared bucket mc mb -p standard/shared/ ${ BUCKETNAME } # There you go! Now you can copy over files or folders! # Create test.txt (if it does not exist) and copy it to your bucket: [ -f test.txt ] || echo \"This is a test\" > test.txt mc cp test.txt standard/ ${ BUCKETNAME } /test.txt Now open the MinIO Web Portal or browse to ~/minio/standard-tenant-1/private to see your test file! mc can do a lot In addition to copying files, mc can do a lot more (like mc ls standard/FIRSTNAME-LASTNAME to list the contents of a bucket). Check out the mc docs or run mc --help for more information. See the example notebooks! There is a template provided for connecting in R , python , or by the command line, provided in jupyter-notebooks/self-serve-storage (also auto-mounted to all jupyter notebook servers in ~/jupyter-notebook ). You can copy-paste and edit these examples! They should suit most of your needs. MinIO Python client \u00b6 You can also connect to MinIO using a python client such as minio or boto3 . import json , minio , re # Get rid of http:// in minio URL http = lambda url : re . sub ( '^https?://' , '' , url ) # Get the MinIO creds with open ( \"/vault/secrets/minio-standard-tenant-1.json\" ) as secrets : d = json . load ( f ) # Create the minio client. s3Client = Minio ( http ( d [ 'MINIO_URL' ]), access_key = d [ 'MINIO_ACCESS_KEY' ], secret_key = d [ 'MINIO_SECRET_KEY' ], secure = False , region = \"us-west-1\" ) But I want to be secure The secure=False thing is a bit misleading. It disables https because this traffic is inside the cluster, and the cluster auto-magically applies mutual TLS. So your applications don't need to worry about https. Your connection is secure by default. But I'm not in us-west? The \"us-west-1\" is an artifact of weird S3 API standards. It is required , even if it doesn't make sense. Other S3-Compliant Methods \u00b6 MinIO is S3 compliant - it uses the same standard as Amazon S3 and other bucket services. Tools designed to use S3 will generally also work with MinIO, for example Python packages and instructions on how to access files from S3. Some examples of this are shown in jupyter-notebooks/self-serve-storage . Bucket Types and Access Scopes \u00b6 The following MinIO tenants (e.g.: separate services) are available: Tenant Speed Cost Access via File Browser Access via mc Access via Web Portal standard-tenant-1 Average Low ~/minio/standard-tenant-1 mc ls standard-tenant-1/$NB_NOTEBOOK link premium-tenant-1 Fast Average ~/minio/premium-tenant-1 mc ls premium-tenant-1/$NB_NOTEBOOK link minimal-tenant1 See note below Average Low Unavailable (see note below) mc ls minimal-tenant1/$NB_NOTEBOOK link premium-tenant1 See note below Fast Average Unavailable (see note below) mc ls premium-tenant1/$NB_NOTEBOOK link minimal-tenant1 and premium-tenant1 are being decommissioned To improve security and stability, minimal-tenant1 and premium-tenant1 have been replaced by standard-tenant-1 and premium-tenant-1 . minimal-tenant1 and premium-tenant1 will for a limited time still be accessible via mc and Web Portal, but will not be accessible via File Browser (due to this causing stability issues for Notebook Servers). It is recommended that you migrate your workloads to the new tenants as soon as possible. A forced migration will occur in future. Note: $NB_NOTEBOOK is an environment variable that contains your namespace You could also just type mc ls standard-tenant-1/john-smith , etc. Accessing all MinIO tenants is the same. The difference between tenants is the storage type behind them: Standard : By default, use this one. It is backed by an SSD and provides a good balance of cost and performance. Premium : Use this if you need high read/write speeds and don't mind paying ~2x the storage cost. These are somewhat faster than the standard storage. Generally if you aren't sure which you need, start with Standard . You can always change your mind if you see your work limited by file transfer speeds. Within each bucket type, everyone has two storage locations they can use, each providing different access scopes: Private Shared Summary Accessible only by someone within your namespace (typically only by you from your own notebook servers/remote desktop, unless you're working in a shared namespace ) Writable only by you, but readable by anyone with access to the platform. Great for sharing public data across teams Mount location in the Notebook Server: ~/minio/standard-tenant-1/private/myfolder/myfile ~/minio/standard-tenant-1/shared/myfolder/myfile Location in mc tool/MinIO portal: firstname-lastname/myfolder/myfile shared/firstname-lastname/myfolder/myfile You can see many directories in the shared MinIO bucket, but you can only write to your own Everyone has read access to all folders in the shared MinIO bucket, but write permissions are always restricted to the owner. Sharing from Private Buckets \u00b6 You can easily share individual files from a private bucket. Just use the \"share\" option for a specific file and you will be provided a link that you can send to a collaborator! Get MinIO Credentials \u00b6 The following methods still work, but you often don't need this anymore because of automation! If you're accessing MinIO from on a Notebook Server or in a Kubeflow Pipeline, these credentials will automatically be loaded into the mc command line tool for you. You can use the mc tool without accessing credentials like shown below (e.g.: just type mc ls standard-tenant-1/MY_NAMESPACE/ ). If you do need credentials, you can still get them from /vault/secrets/minio-* To access your MinIO buckets programmatically (for example through the mc command line tool , or via Python or R) you require personal MinIO credentials. Methods for obtaining these credentials are outlined here. Method 1: Get Credentials from Vault \u00b6 To get your MinIO credentials, you can use the Vault . Select method OIDC, leave Role blank and Sign in with OIDC Provider. Run the following command in the terminal located at the top right corner: # Replace standard with premium depending on your Bucket type read minio_standard_tenant_1/keys/profile-yourfirstname-yourlastname Method 2: Get Credentials from Running Notebook Server \u00b6 Open a terminal in your Notebook and run: cat /vault/secrets/minio-standard-tenant-1 # Output: # export MINIO_URL=\"http://minio.minio-standard-tenant-1 ...\" # export MINIO_ACCESS_KEY=\"...\" # export MINIO_SECRET_KEY=\"...\" Pricing \u00b6 Pricing models are tentative and may change As of writing, pricing is covered by the platform for initial users. This guidance explains how things are expected to be priced priced in future, but this may change. Exact pricing for MinIO resources are hard to state because they're prorated across multiple users. In general though, the underlying storage is provided by Azure Manage Disks and they give a rough guide for MinIO storage cost based on the MinIO instance: premium-tenant-1: See Premium SSD Managed Disks standard-tenant-1: See Standard SSD Managed Disks Typically 50% the cost of premium-tenant-1","title":"Storage"},{"location":"Storage/#overview","text":"The platform provides several types of storage: Disk (also called Volumes on the Notebook Server creation screen) Bucket (\"Blob\" or S3 storage, provided through MinIO) Data Lakes (coming soon) Depending on your use case, either disk or bucket may be most suitable: Type Simultaneous Users Speed Total size Sharable with Other Users Disk One machine/notebook server at a time Fastest (throughput and latency) <=512GB total per drive No Bucket Simultaneous access from many machines/notebook servers at the same time Fast-ish (Fast download, modest upload, modest latency) Infinite (within reason) Yes If you're unsure which to choose, don't sweat it These are guidelines, not an exact science - pick what sounds best now and run with it. The best choice for a complicated usage is non-obvious and often takes hands-on experience, so just trying something will help. For most situations both options work well even if they're not perfect, and remember that data can always be copied later if you change your mind.","title":"Overview"},{"location":"Storage/#disks","text":"","title":"Disks"},{"location":"Storage/#overview_1","text":"Disks are the familiar hard drive style file systems you're used to, provided to you from fast solid state drives (SSDs)! When creating your notebook server, you request them by adding Data Volumes to your notebook server (pictured below, with Type = New ). They are automatically mounted at the directory ( Mount Point ) you choose, and serve as a simple and reliable way to preserve data attached to a Notebook Server. When you delete your Notebook Server, your disks are not deleted . This let's you reuse that same disk (with all its contents) on a new Notebook Server later (as shown above with Type = Existing and the Name set to the volume you want to reuse). If you're done with the disk and it's contents, delete it . You pay for all disks you own, whether they're attached to a Notebook Server or not As soon as you create a disk, you're paying for it until it is deleted , even if it's original Notebook Server is deleted. See Deleting Disk Storage for more info","title":"Overview"},{"location":"Storage/#pricing","text":"Pricing models are tentative and may change As of writing, pricing is covered by the platform for initial users. This guidance explains how things are expected to be priced priced in future, but this may change. When mounting a disk, you get an Azure Managed Disk . The Premium SSD Managed Disks pricing shows the cost per disk based on size. Note that you pay for the size of disk requested, not the amount of space you are currently using. Tips to minimize costs As disks can be attached to a Notebook Server and reused, a typical usage pattern could be: At 9AM, create a Notebook Server (request 2CPU/8GB RAM and a 32GB attached disk) Do work throughout the day, saving results to the attached disk At 5PM, shut down your Notebook Server to avoid paying for it overnight NOTE: The attached disk is not destroyed by this action At 9AM the next day, create a new Notebook Server and attach your existing disk Continue your work... This keeps all your work safe without paying for the computer when you're not using it","title":"Pricing"},{"location":"Storage/#deleting-disk-storage","text":"To see your disks, check the Notebook Volumes section of the Notebook Server page (shown below). You can delete any unattached disk (orange icon on the left) by clicking the trash can icon.","title":"Deleting Disk Storage"},{"location":"Storage/#buckets-via-minio","text":"(2021-02-12: To improve stability and security, we have implemented new MinIO instances to replace the originals. We strongly suggest you use the new instances as soon as possible See Bucket Types and Access Scopes ) for more details Buckets are slightly more complicated, but they are good at three things: Large amounts of data Buckets can be huge: way bigger than hard drives. And they are still fast. Accessible by multiple consumers at once You can access the same data source from multiple Notebook Servers and pipelines at the same time without needing to duplicate the data Sharing You can share files from a bucket by sharing a URL that you can get through a simple web interface. This is great for sharing data with people outside of your workspace. There are several implementations of","title":"Buckets (via MinIO)"},{"location":"Storage/#accessing-your-bucket","text":"There are multiple ways to upload and download data from your MinIO buckets: Mounted folders on a Notebook Server MinIO web portal MinIO command line tool mc Other S3-Compliant Methods Different access methods have strengths and weaknesses, but the data all goes to the same place You can: Upload a file using the mounted folder on a notebook server Rename that file using the web portal Download that file using the mc tool and all steps will be working on the same file . This lets you mix and match based on what is easiest for your tasks. There are two different MinIO services The examples below use the standard-tenant-1 instance of MinIO, but there is also a second instance: premium-tenant-1 . See Bucket Types and Access Scopes for more details. To use premium-tenant-1 in these examples, just substitute that name in for standard-tenant-1 .","title":"Accessing your Bucket"},{"location":"Storage/#minio-mounted-folders-on-a-notebook-server","text":"Automatically, all Notebook Servers have your MinIO storage mounted as directories. This is located in ~/minio : These folders can be used like any other - you can copy files to/from using the file browser, write from Python/R, etc. The only difference is that the data is being stored in the MinIO service rather than on a local disk (and is thus accessible wherever you can access your MinIO bucket, rather than just from the Notebook Server it is attached to like a Disk ). Files copied into a mounted MinIO folder might take a few moments to be readable When you copy files into a MinIO folder, they are immediately stored and accessible in MinIO (e.g.: you can immediately see them in the Web Portal ). But, new files may take a few moments for the mounting service to notice and serve them in the mounted folder. If your use case needs access to these files immediately after copying them, try the other read methods ( the mc tool or Other S3 Compliant Methods ).","title":"MinIO Mounted Folders on a Notebook Server"},{"location":"Storage/#minio-web-portal","text":"The MinIO service can be accessible through a web portal . To sign in using your existing credentials, use the \"Log in with OpenID\" button. From this portal you can browse to your personal bucket, which has the same name as your Kubeflow namespace (likely firstname-lastname ): This lets you browse, upload/download, delete, or share files.","title":"MinIO Web Portal"},{"location":"Storage/#minio-command-line-tool","text":"MinIO provides the command line tool mc to access your data from a terminal. mc can communicate with one or more MinIO instances to let you upload/download files. For example: To run the below example yourself, replace BUCKETNAME 's value with your first/last name. For example: BUCKETNAME=john-smith . #!/bin/sh # The name of your bucket. This MUST be the same as your namespace's name. # Typically this is \"firstname-lastname\", but it might be different if working in a shared namespace BUCKETNAME = firstname-lastname # Get your personal credentials for the \"minio-standard-tenant-1\" MinIO instance # (this initializes $MINIO_URL, $MINIO_ACCESS_KEY, and $MINIO_SECRET_KEY environment variables) source /vault/secrets/minio-standard-tenant-1 # Create a MinIO alias (called \"standard\") for \"standard\" using your credentials mc config host add standard $MINIO_URL $MINIO_ACCESS_KEY $MINIO_SECRET_KEY # Create a bucket under your name # NOTE: You can *only* create buckets named the same as your namespace. Any # other name will be rejected. # Private bucket (\"mb\" = \"make bucket\") mc mb -p standard/ ${ BUCKETNAME } # Shared bucket mc mb -p standard/shared/ ${ BUCKETNAME } # There you go! Now you can copy over files or folders! # Create test.txt (if it does not exist) and copy it to your bucket: [ -f test.txt ] || echo \"This is a test\" > test.txt mc cp test.txt standard/ ${ BUCKETNAME } /test.txt Now open the MinIO Web Portal or browse to ~/minio/standard-tenant-1/private to see your test file! mc can do a lot In addition to copying files, mc can do a lot more (like mc ls standard/FIRSTNAME-LASTNAME to list the contents of a bucket). Check out the mc docs or run mc --help for more information. See the example notebooks! There is a template provided for connecting in R , python , or by the command line, provided in jupyter-notebooks/self-serve-storage (also auto-mounted to all jupyter notebook servers in ~/jupyter-notebook ). You can copy-paste and edit these examples! They should suit most of your needs.","title":"MinIO Command Line Tool"},{"location":"Storage/#minio-python-client","text":"You can also connect to MinIO using a python client such as minio or boto3 . import json , minio , re # Get rid of http:// in minio URL http = lambda url : re . sub ( '^https?://' , '' , url ) # Get the MinIO creds with open ( \"/vault/secrets/minio-standard-tenant-1.json\" ) as secrets : d = json . load ( f ) # Create the minio client. s3Client = Minio ( http ( d [ 'MINIO_URL' ]), access_key = d [ 'MINIO_ACCESS_KEY' ], secret_key = d [ 'MINIO_SECRET_KEY' ], secure = False , region = \"us-west-1\" ) But I want to be secure The secure=False thing is a bit misleading. It disables https because this traffic is inside the cluster, and the cluster auto-magically applies mutual TLS. So your applications don't need to worry about https. Your connection is secure by default. But I'm not in us-west? The \"us-west-1\" is an artifact of weird S3 API standards. It is required , even if it doesn't make sense.","title":"MinIO Python client"},{"location":"Storage/#other-s3-compliant-methods","text":"MinIO is S3 compliant - it uses the same standard as Amazon S3 and other bucket services. Tools designed to use S3 will generally also work with MinIO, for example Python packages and instructions on how to access files from S3. Some examples of this are shown in jupyter-notebooks/self-serve-storage .","title":"Other S3-Compliant Methods"},{"location":"Storage/#bucket-types-and-access-scopes","text":"The following MinIO tenants (e.g.: separate services) are available: Tenant Speed Cost Access via File Browser Access via mc Access via Web Portal standard-tenant-1 Average Low ~/minio/standard-tenant-1 mc ls standard-tenant-1/$NB_NOTEBOOK link premium-tenant-1 Fast Average ~/minio/premium-tenant-1 mc ls premium-tenant-1/$NB_NOTEBOOK link minimal-tenant1 See note below Average Low Unavailable (see note below) mc ls minimal-tenant1/$NB_NOTEBOOK link premium-tenant1 See note below Fast Average Unavailable (see note below) mc ls premium-tenant1/$NB_NOTEBOOK link minimal-tenant1 and premium-tenant1 are being decommissioned To improve security and stability, minimal-tenant1 and premium-tenant1 have been replaced by standard-tenant-1 and premium-tenant-1 . minimal-tenant1 and premium-tenant1 will for a limited time still be accessible via mc and Web Portal, but will not be accessible via File Browser (due to this causing stability issues for Notebook Servers). It is recommended that you migrate your workloads to the new tenants as soon as possible. A forced migration will occur in future. Note: $NB_NOTEBOOK is an environment variable that contains your namespace You could also just type mc ls standard-tenant-1/john-smith , etc. Accessing all MinIO tenants is the same. The difference between tenants is the storage type behind them: Standard : By default, use this one. It is backed by an SSD and provides a good balance of cost and performance. Premium : Use this if you need high read/write speeds and don't mind paying ~2x the storage cost. These are somewhat faster than the standard storage. Generally if you aren't sure which you need, start with Standard . You can always change your mind if you see your work limited by file transfer speeds. Within each bucket type, everyone has two storage locations they can use, each providing different access scopes: Private Shared Summary Accessible only by someone within your namespace (typically only by you from your own notebook servers/remote desktop, unless you're working in a shared namespace ) Writable only by you, but readable by anyone with access to the platform. Great for sharing public data across teams Mount location in the Notebook Server: ~/minio/standard-tenant-1/private/myfolder/myfile ~/minio/standard-tenant-1/shared/myfolder/myfile Location in mc tool/MinIO portal: firstname-lastname/myfolder/myfile shared/firstname-lastname/myfolder/myfile You can see many directories in the shared MinIO bucket, but you can only write to your own Everyone has read access to all folders in the shared MinIO bucket, but write permissions are always restricted to the owner.","title":"Bucket Types and Access Scopes"},{"location":"Storage/#sharing-from-private-buckets","text":"You can easily share individual files from a private bucket. Just use the \"share\" option for a specific file and you will be provided a link that you can send to a collaborator!","title":"Sharing from Private Buckets"},{"location":"Storage/#get-minio-credentials","text":"The following methods still work, but you often don't need this anymore because of automation! If you're accessing MinIO from on a Notebook Server or in a Kubeflow Pipeline, these credentials will automatically be loaded into the mc command line tool for you. You can use the mc tool without accessing credentials like shown below (e.g.: just type mc ls standard-tenant-1/MY_NAMESPACE/ ). If you do need credentials, you can still get them from /vault/secrets/minio-* To access your MinIO buckets programmatically (for example through the mc command line tool , or via Python or R) you require personal MinIO credentials. Methods for obtaining these credentials are outlined here.","title":"Get MinIO Credentials"},{"location":"Storage/#method-1-get-credentials-from-vault","text":"To get your MinIO credentials, you can use the Vault . Select method OIDC, leave Role blank and Sign in with OIDC Provider. Run the following command in the terminal located at the top right corner: # Replace standard with premium depending on your Bucket type read minio_standard_tenant_1/keys/profile-yourfirstname-yourlastname","title":"Method 1: Get Credentials from Vault"},{"location":"Storage/#method-2-get-credentials-from-running-notebook-server","text":"Open a terminal in your Notebook and run: cat /vault/secrets/minio-standard-tenant-1 # Output: # export MINIO_URL=\"http://minio.minio-standard-tenant-1 ...\" # export MINIO_ACCESS_KEY=\"...\" # export MINIO_SECRET_KEY=\"...\"","title":"Method 2: Get Credentials from Running Notebook Server"},{"location":"Storage/#pricing_1","text":"Pricing models are tentative and may change As of writing, pricing is covered by the platform for initial users. This guidance explains how things are expected to be priced priced in future, but this may change. Exact pricing for MinIO resources are hard to state because they're prorated across multiple users. In general though, the underlying storage is provided by Azure Manage Disks and they give a rough guide for MinIO storage cost based on the MinIO instance: premium-tenant-1: See Premium SSD Managed Disks standard-tenant-1: See Standard SSD Managed Disks Typically 50% the cost of premium-tenant-1","title":"Pricing"},{"location":"1-Experiments/Jupyter/","text":"Jupyter \u00b6 Friendly R and Python experience \u00b6 Jupyter gives you notebooks to write your code and make visualizations. You can quickly iterate, visualize, and share your analyses. Because it's running on a server (that you set up in the last section) you can do really big analyses on centralized hardware! Adding as much horsepower as you need! And because it's on the cloud, you can share it with your colleagues too. Explore your data \u00b6 Jupyter comes with a number of features (and we can add more) Integrated visuals within your notebook Data volume for storing your data You can share your workspace with colleagues. IDE in the browser \u00b6 Create for exploring, and also great for writing code Linting and a debugger Git integration Built in Terminal Light/Dark theme (change settings at the top) More information on Jupyter here Get started with the examples \u00b6 When you started your server, it got loaded with a bunch of example notebooks. Great notebooks to start with are R/01-R-Notebook-Demo.ipynb , or the notebooks in scikitlearn . pytorch and tensorflow are great if you are familiar with machine learning. The mapreduce-pipeline and ai-pipeline are more advanced. Some notebooks only work in certain server versions For instance, gdal is only in the geomatics image. So if you use another image then a notebook using gdal might not work. Adding software \u00b6 You do not have sudo in Jupyter, but you can use conda install --use-local your_package_name or pip install --user your_package_name Don't forget to restart your Jupyter kernel afterwards, to make new packages available. Make sure to restart the Jupyter kernel after installing new software If you install software in a terminal, but your Jupyter kernel was already running, then it will not be updated. Is there something that you can't install? If you need something installed, reach us or open a GitHub issue . We can add it to the default software. Getting Data in and out of Jupyter \u00b6 You can upload and download data to/from JupyterHub directly in the menu. There is an upload button at the top, and you can right-click most files or folders to download them. Shareable \"Bucket\" storage \u00b6 There is also a mounted minio folder in your home directory, which holds files in MinIO . Refer to the Storage Section for details.","title":"Jupyter"},{"location":"1-Experiments/Jupyter/#jupyter","text":"","title":"Jupyter"},{"location":"1-Experiments/Jupyter/#friendly-r-and-python-experience","text":"Jupyter gives you notebooks to write your code and make visualizations. You can quickly iterate, visualize, and share your analyses. Because it's running on a server (that you set up in the last section) you can do really big analyses on centralized hardware! Adding as much horsepower as you need! And because it's on the cloud, you can share it with your colleagues too.","title":"Friendly R and Python experience"},{"location":"1-Experiments/Jupyter/#explore-your-data","text":"Jupyter comes with a number of features (and we can add more) Integrated visuals within your notebook Data volume for storing your data You can share your workspace with colleagues.","title":"Explore your data"},{"location":"1-Experiments/Jupyter/#ide-in-the-browser","text":"Create for exploring, and also great for writing code Linting and a debugger Git integration Built in Terminal Light/Dark theme (change settings at the top) More information on Jupyter here","title":"IDE in the browser"},{"location":"1-Experiments/Jupyter/#get-started-with-the-examples","text":"When you started your server, it got loaded with a bunch of example notebooks. Great notebooks to start with are R/01-R-Notebook-Demo.ipynb , or the notebooks in scikitlearn . pytorch and tensorflow are great if you are familiar with machine learning. The mapreduce-pipeline and ai-pipeline are more advanced. Some notebooks only work in certain server versions For instance, gdal is only in the geomatics image. So if you use another image then a notebook using gdal might not work.","title":"Get started with the examples"},{"location":"1-Experiments/Jupyter/#adding-software","text":"You do not have sudo in Jupyter, but you can use conda install --use-local your_package_name or pip install --user your_package_name Don't forget to restart your Jupyter kernel afterwards, to make new packages available. Make sure to restart the Jupyter kernel after installing new software If you install software in a terminal, but your Jupyter kernel was already running, then it will not be updated. Is there something that you can't install? If you need something installed, reach us or open a GitHub issue . We can add it to the default software.","title":"Adding software"},{"location":"1-Experiments/Jupyter/#getting-data-in-and-out-of-jupyter","text":"You can upload and download data to/from JupyterHub directly in the menu. There is an upload button at the top, and you can right-click most files or folders to download them.","title":"Getting Data in and out of Jupyter"},{"location":"1-Experiments/Jupyter/#shareable-bucket-storage","text":"There is also a mounted minio folder in your home directory, which holds files in MinIO . Refer to the Storage Section for details.","title":"Shareable \"Bucket\" storage"},{"location":"1-Experiments/Kubeflow/","text":"Getting started with Kubeflow \u00b6 What does Kubeflow do? \u00b6 Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and create shared workspaces for your team. Let's get started! Create a Server \u00b6 Log into Kubeflow \u00b6 Log into the Azure Portal using your Cloud Credentials You have to login to the azure portal using your StatCan credentials . first.lastname@cloud.statcan.ca . You can do that using the Azure portal . Log into Kubeflow Navigate to the Jupyter Servers tab Then click + New Server Configuring your server \u00b6 You will get a template to create your notebook server. Note: the name must be lowercase letters with hyphens. No spaces, and no underscores. You'll need to choose an image You will probably want one of Machine Learning Geomatics Minimal If you want to use a GPU, check if the image says cpu or gpu . CPU and Memory \u00b6 At the time of writing (April 21, 2020) there are two types of computers in the cluster CPU: D16s v3 (16 CPU cores, 64 GiB memory) GPU: NC6s_v3 (6 CPU cores, 112 GiB memory, 1 GPU) Because of this, if you request too much RAM or too many CPUs, it may be hard or impossible to satisfy your request. In the future (possibly when you read this) there may be larger machines made available, so you may have looser restrictions. Use GPU machines responsibly There are fewer GPU machines than CPU machines, so use them responsibly. Storing your data \u00b6 You'll want to create a data volume! You'll be able to save your work here, and if you shut down your server, you'll be able to just remount your old data by entering the name of your old disk. It is important that you remember the volume's name. Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume. And... Create!!! \u00b6 If you're satisfied with the settings, you can now create the server! It may take a few minutes to spin up depending on the resources you asked for. (GPUs take longer.) Your server is running If all goes well, your server should be running!!! You will now have the option to connect, and try out Jupyter! Share your workspace \u00b6 In Kubeflow every user has a namespace . Your namespace belongs to you, and it's where all your resources live. If you want to collaborate with someone you need to share a namespace. So you can do that either by sharing your own namespace, or more preferably, by creating a team namespace . The link to create a new namespace is in the \u22ee menu on the Kubeflow section of the portal . Manage contributors \u00b6 You can add or remove people from a namespace you already own through the Manage Contributors menu in Kubeflow. Now you and your colleagues can share access to a server! Now you can share a server with colleagues! Try it out! For more details on collaboration on the platform, see Collaboration .","title":"Kubeflow"},{"location":"1-Experiments/Kubeflow/#getting-started-with-kubeflow","text":"","title":"Getting started with Kubeflow"},{"location":"1-Experiments/Kubeflow/#what-does-kubeflow-do","text":"Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and create shared workspaces for your team. Let's get started!","title":"What does Kubeflow do?"},{"location":"1-Experiments/Kubeflow/#create-a-server","text":"","title":"Create a Server"},{"location":"1-Experiments/Kubeflow/#log-into-kubeflow","text":"Log into the Azure Portal using your Cloud Credentials You have to login to the azure portal using your StatCan credentials . first.lastname@cloud.statcan.ca . You can do that using the Azure portal . Log into Kubeflow Navigate to the Jupyter Servers tab Then click + New Server","title":"Log into Kubeflow"},{"location":"1-Experiments/Kubeflow/#configuring-your-server","text":"You will get a template to create your notebook server. Note: the name must be lowercase letters with hyphens. No spaces, and no underscores. You'll need to choose an image You will probably want one of Machine Learning Geomatics Minimal If you want to use a GPU, check if the image says cpu or gpu .","title":"Configuring your server"},{"location":"1-Experiments/Kubeflow/#cpu-and-memory","text":"At the time of writing (April 21, 2020) there are two types of computers in the cluster CPU: D16s v3 (16 CPU cores, 64 GiB memory) GPU: NC6s_v3 (6 CPU cores, 112 GiB memory, 1 GPU) Because of this, if you request too much RAM or too many CPUs, it may be hard or impossible to satisfy your request. In the future (possibly when you read this) there may be larger machines made available, so you may have looser restrictions. Use GPU machines responsibly There are fewer GPU machines than CPU machines, so use them responsibly.","title":"CPU and Memory"},{"location":"1-Experiments/Kubeflow/#storing-your-data","text":"You'll want to create a data volume! You'll be able to save your work here, and if you shut down your server, you'll be able to just remount your old data by entering the name of your old disk. It is important that you remember the volume's name. Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume.","title":"Storing your data"},{"location":"1-Experiments/Kubeflow/#and-create","text":"If you're satisfied with the settings, you can now create the server! It may take a few minutes to spin up depending on the resources you asked for. (GPUs take longer.) Your server is running If all goes well, your server should be running!!! You will now have the option to connect, and try out Jupyter!","title":"And... Create!!!"},{"location":"1-Experiments/Kubeflow/#share-your-workspace","text":"In Kubeflow every user has a namespace . Your namespace belongs to you, and it's where all your resources live. If you want to collaborate with someone you need to share a namespace. So you can do that either by sharing your own namespace, or more preferably, by creating a team namespace . The link to create a new namespace is in the \u22ee menu on the Kubeflow section of the portal .","title":"Share your workspace"},{"location":"1-Experiments/Kubeflow/#manage-contributors","text":"You can add or remove people from a namespace you already own through the Manage Contributors menu in Kubeflow. Now you and your colleagues can share access to a server! Now you can share a server with colleagues! Try it out! For more details on collaboration on the platform, see Collaboration .","title":"Manage contributors"},{"location":"1-Experiments/MLflow/","text":"MLflow for model tracking \u00b6 MLflow is an open source platform for managing the Machine Learning lifecycle. It is a \"Model Registry\" for storing your machine learning models and associated metrics. You can use the web interface to examine your models, and you can use its REST API to register your models from Python, using the mlflow pip package .","title":"MLflow"},{"location":"1-Experiments/MLflow/#mlflow-for-model-tracking","text":"MLflow is an open source platform for managing the Machine Learning lifecycle. It is a \"Model Registry\" for storing your machine learning models and associated metrics. You can use the web interface to examine your models, and you can use its REST API to register your models from Python, using the mlflow pip package .","title":"MLflow for model tracking"},{"location":"1-Experiments/RStudio/","text":"R-Studio \u00b6 You can use the r-studio-cpu image to get an RStudio environment! You can install R or python packages with conda or install.packages() . R-Shiny \u00b6 You can use Shiny , too! And the dashboard appears as a pop-up window.","title":"RStudio"},{"location":"1-Experiments/RStudio/#r-studio","text":"You can use the r-studio-cpu image to get an RStudio environment! You can install R or python packages with conda or install.packages() .","title":"R-Studio"},{"location":"1-Experiments/RStudio/#r-shiny","text":"You can use Shiny , too! And the dashboard appears as a pop-up window.","title":"R-Shiny"},{"location":"1-Experiments/Remote-Desktop/","text":"Remote Desktop \u00b6 What is Remote Desktop? \u00b6 Remote Desktop provides an in-browser GUI Ubuntu desktop experience as well as quick access to supporting tools. The operating system is Ubuntu 18.04 with the XFCE desktop environment. Versions \u00b6 Two versions of Remote Desktop are available. R includes R and RStudio. Geomatics extends R with QGIS and various supporting libraries. You may further customize your Remote Desktop workspace to suit your specific needs and preferences. Customization \u00b6 pip , conda , npm and yarn are available to install various packages. Accessing the Remote Desktop \u00b6 To launch the Remote Desktop or any of its supporting tools, create a Notebook Server in Kubeflow and select one of the available versions in the image dropdown. Then, click Connect to access the landing page. Remote Desktop brings you to the Desktop GUI through a noVNC session. Click on the < on the left side of the screen to expand a panel with options such as fullscreen and clipboard access. Legacy Interface: Jupyter Notebook \u00b6 Jupyter Notebook is a legacy interface for managing Jupyter Notebooks until storage integration with the JupyterLab images becomes available. You can access other tools and interfaces with the Open Tool button in the top right corner, or by returning to the landing page. In-browser Tools \u00b6 VS Code \u00b6 VS Code brings you to the Visual Studio Code IDE. Netdata \u00b6 Netdata delivers in-depth interactive resource monitoring. Charts can be panned by dragging them. You can also zoom in and out with SHIFT + mouse wheel , or zoom to a selection by holding down SHIFT while dragging. Filebrowser \u00b6 Filebrowser can be used to quickly explore the file system of the Remote Desktop as well as to transfer files between the Remote Desktop and your computer. Access Port \u00b6 Finally, Access Port provides in-browser access to anything served on a specified port inside the Remote Desktop (internally accessible at e.g. localhost:8059 ). As with the other tools, this opens in a separate page in your browser. Your namespace collaborators can also access this page if you send them the URL. Example: Accessing the Supervisor API at port 8059 Footnotes \u00b6 Remote Desktop is based on ml-tooling/ml-workspace .","title":"Remote Desktop"},{"location":"1-Experiments/Remote-Desktop/#remote-desktop","text":"","title":"Remote Desktop"},{"location":"1-Experiments/Remote-Desktop/#what-is-remote-desktop","text":"Remote Desktop provides an in-browser GUI Ubuntu desktop experience as well as quick access to supporting tools. The operating system is Ubuntu 18.04 with the XFCE desktop environment.","title":"What is Remote Desktop?"},{"location":"1-Experiments/Remote-Desktop/#versions","text":"Two versions of Remote Desktop are available. R includes R and RStudio. Geomatics extends R with QGIS and various supporting libraries. You may further customize your Remote Desktop workspace to suit your specific needs and preferences.","title":"Versions"},{"location":"1-Experiments/Remote-Desktop/#customization","text":"pip , conda , npm and yarn are available to install various packages.","title":"Customization"},{"location":"1-Experiments/Remote-Desktop/#accessing-the-remote-desktop","text":"To launch the Remote Desktop or any of its supporting tools, create a Notebook Server in Kubeflow and select one of the available versions in the image dropdown. Then, click Connect to access the landing page. Remote Desktop brings you to the Desktop GUI through a noVNC session. Click on the < on the left side of the screen to expand a panel with options such as fullscreen and clipboard access.","title":"Accessing the Remote Desktop"},{"location":"1-Experiments/Remote-Desktop/#legacy-interface-jupyter-notebook","text":"Jupyter Notebook is a legacy interface for managing Jupyter Notebooks until storage integration with the JupyterLab images becomes available. You can access other tools and interfaces with the Open Tool button in the top right corner, or by returning to the landing page.","title":"Legacy Interface: Jupyter Notebook"},{"location":"1-Experiments/Remote-Desktop/#in-browser-tools","text":"","title":"In-browser Tools"},{"location":"1-Experiments/Remote-Desktop/#vs-code","text":"VS Code brings you to the Visual Studio Code IDE.","title":"VS Code"},{"location":"1-Experiments/Remote-Desktop/#netdata","text":"Netdata delivers in-depth interactive resource monitoring. Charts can be panned by dragging them. You can also zoom in and out with SHIFT + mouse wheel , or zoom to a selection by holding down SHIFT while dragging.","title":"Netdata"},{"location":"1-Experiments/Remote-Desktop/#filebrowser","text":"Filebrowser can be used to quickly explore the file system of the Remote Desktop as well as to transfer files between the Remote Desktop and your computer.","title":"Filebrowser"},{"location":"1-Experiments/Remote-Desktop/#access-port","text":"Finally, Access Port provides in-browser access to anything served on a specified port inside the Remote Desktop (internally accessible at e.g. localhost:8059 ). As with the other tools, this opens in a separate page in your browser. Your namespace collaborators can also access this page if you send them the URL. Example: Accessing the Supervisor API at port 8059","title":"Access Port"},{"location":"1-Experiments/Remote-Desktop/#footnotes","text":"Remote Desktop is based on ml-tooling/ml-workspace .","title":"Footnotes"},{"location":"2-Publishing/Accessing-Published-Content/","text":"","title":"Accessing Published Content"},{"location":"2-Publishing/Custom/","text":"Custom Web Apps \u00b6 We can deploy anything as long as it's open source and we can put it in a Docker container. For instance, Node.js apps, Flask or Dash apps. Etc. See the source code for this app We just push these kinds of applications through GitHub into the server. The source for the above app is StatCan/covid19 How to get your app hosted \u00b6 If you already have a web app in a git repository then, as soon as it's containerized, we can fork the Git repository into the StatCan GitHub repository and point a URL to it. To update it, you'll just interact with the StatCan GitHub repository with Pull Requests. Contact us if you have questions.","title":"Custom"},{"location":"2-Publishing/Custom/#custom-web-apps","text":"We can deploy anything as long as it's open source and we can put it in a Docker container. For instance, Node.js apps, Flask or Dash apps. Etc. See the source code for this app We just push these kinds of applications through GitHub into the server. The source for the above app is StatCan/covid19","title":"Custom Web Apps"},{"location":"2-Publishing/Custom/#how-to-get-your-app-hosted","text":"If you already have a web app in a git repository then, as soon as it's containerized, we can fork the Git repository into the StatCan GitHub repository and point a URL to it. To update it, you'll just interact with the StatCan GitHub repository with Pull Requests. Contact us if you have questions.","title":"How to get your app hosted"},{"location":"2-Publishing/Dash/","text":"Getting Started with Dash! \u00b6 For data visualization tools, we will be using Dash. Dash is a great tool used by many for data analysis, data exploration, visualization, modelling, instrument control, and reporting. The following example demonstrates a highly reactive and customised Dash app with little code. Running your Notebook Server and accessing the port When running any tool from your Jupyter Notebook that posts a website to a port, you will not be able to simply access it from http://localhost:5000/ as normally suggested in the output upon running the web-app. To access the web server you will need to use the base URL. In your notebook terminal run: python echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/ Data Visualization with Dash \u00b6 Dash makes it simple to build an interactive GUI around your data analysis code. This is an example of a Layout With Figure and Slider from Dash . # required installations if not already installed pip3 install dash == 1.16 . 3 pip3 install pandas # app.py #!/usr/bin/env python3 import dash import dash_core_components as dcc import dash_html_components as html from dash.dependencies import Input , Output import plotly.express as px import pandas as pd df = pd . read_csv ( 'https://raw.githubusercontent.com/plotly/datasets/master/gapminderDataFiveYear.csv' ) external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) app . layout = html . Div ([ dcc . Graph ( id = 'graph-with-slider' ), dcc . Slider ( id = 'year-slider' , min = df [ 'year' ] . min (), max = df [ 'year' ] . max (), value = df [ 'year' ] . min (), marks = { str ( year ): str ( year ) for year in df [ 'year' ] . unique ()}, step = None ) ]) @app . callback ( Output ( 'graph-with-slider' , 'figure' ), [ Input ( 'year-slider' , 'value' )]) def update_figure ( selected_year ): filtered_df = df [ df . year == selected_year ] fig = px . scatter ( filtered_df , x = \"gdpPercap\" , y = \"lifeExp\" , size = \"pop\" , color = \"continent\" , hover_name = \"country\" , log_x = True , size_max = 55 ) fig . update_layout ( transition_duration = 500 ) return fig if __name__ == '__main__' : app . run_server ( debug = True ) Run your app \u00b6 python app . py # or you can use: export FLASK_APP = app . py flask run","title":"Dash"},{"location":"2-Publishing/Dash/#getting-started-with-dash","text":"For data visualization tools, we will be using Dash. Dash is a great tool used by many for data analysis, data exploration, visualization, modelling, instrument control, and reporting. The following example demonstrates a highly reactive and customised Dash app with little code. Running your Notebook Server and accessing the port When running any tool from your Jupyter Notebook that posts a website to a port, you will not be able to simply access it from http://localhost:5000/ as normally suggested in the output upon running the web-app. To access the web server you will need to use the base URL. In your notebook terminal run: python echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/","title":"Getting Started with Dash!"},{"location":"2-Publishing/Dash/#data-visualization-with-dash","text":"Dash makes it simple to build an interactive GUI around your data analysis code. This is an example of a Layout With Figure and Slider from Dash . # required installations if not already installed pip3 install dash == 1.16 . 3 pip3 install pandas # app.py #!/usr/bin/env python3 import dash import dash_core_components as dcc import dash_html_components as html from dash.dependencies import Input , Output import plotly.express as px import pandas as pd df = pd . read_csv ( 'https://raw.githubusercontent.com/plotly/datasets/master/gapminderDataFiveYear.csv' ) external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) app . layout = html . Div ([ dcc . Graph ( id = 'graph-with-slider' ), dcc . Slider ( id = 'year-slider' , min = df [ 'year' ] . min (), max = df [ 'year' ] . max (), value = df [ 'year' ] . min (), marks = { str ( year ): str ( year ) for year in df [ 'year' ] . unique ()}, step = None ) ]) @app . callback ( Output ( 'graph-with-slider' , 'figure' ), [ Input ( 'year-slider' , 'value' )]) def update_figure ( selected_year ): filtered_df = df [ df . year == selected_year ] fig = px . scatter ( filtered_df , x = \"gdpPercap\" , y = \"lifeExp\" , size = \"pop\" , color = \"continent\" , hover_name = \"country\" , log_x = True , size_max = 55 ) fig . update_layout ( transition_duration = 500 ) return fig if __name__ == '__main__' : app . run_server ( debug = True )","title":"Data Visualization with Dash"},{"location":"2-Publishing/Dash/#run-your-app","text":"python app . py # or you can use: export FLASK_APP = app . py flask run","title":"Run your app"},{"location":"2-Publishing/Datasette/","text":"Explore your data with Datasette \u00b6 Datasette is an instant JSON API for your SQLite databases allowing you to explore the DB and run SQL queries in a more interactive way. You can find a list of example datasettes here . The Datasette Ecosystem There are all sorts of tools for converting data to and from sqlite here . For example, you can load shapefiles into sqlite, or create Vega plots from a sqlite database. SQLite works well with R , Python , and many other tools. Example Datasette \u00b6 Screenshots from global-power-plants , you can preview and explore the data in the browser, either with clicks or SQL queries. You can even explore maps within the tool! Starting Datasette \u00b6 To view your own database in your Jupyter Notebook, create the following bash file in your project directory and run with ./start.sh . Access the web server using the base URL with the port number you are using in the below file. start.sh #!/bin/bash # This script just starts Datasette with the correct URL, so # that you can use it within kubeflow. # Get an example database wget https://github.com/StatCan/R-notebooks/raw/master/database-connections/latin_phrases.db # If you have your own database, you can change this line! DATABASE = latin_phrases.db export BASE_URL = \"https://kubeflow.covid.cloud.statcan.ca ${ JUPYTER_SERVER_URL : 19 } proxy/8001/\" echo \"Base url: ${ BASE_URL } \" datasette $DATABASE --cors --config max_returned_rows:100000 --config sql_time_limit_ms:5500 --config base_url: ${ BASE_URL } Check out this video tutorial One user of the platform used Datasette along with a javascript dashboard. See it video for a Running your Notebook Server and accessing the port When running any tool from your Jupyter Notebook that posts a website to a port, you will not be able to simply access it from http://localhost:5000/ as normally suggested in the output upon running the web-app. To access the web server you will need to use the base URL. In your notebook terminal run: python echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/","title":"Datasette"},{"location":"2-Publishing/Datasette/#explore-your-data-with-datasette","text":"Datasette is an instant JSON API for your SQLite databases allowing you to explore the DB and run SQL queries in a more interactive way. You can find a list of example datasettes here . The Datasette Ecosystem There are all sorts of tools for converting data to and from sqlite here . For example, you can load shapefiles into sqlite, or create Vega plots from a sqlite database. SQLite works well with R , Python , and many other tools.","title":"Explore your data with Datasette"},{"location":"2-Publishing/Datasette/#example-datasette","text":"Screenshots from global-power-plants , you can preview and explore the data in the browser, either with clicks or SQL queries. You can even explore maps within the tool!","title":"Example Datasette"},{"location":"2-Publishing/Datasette/#starting-datasette","text":"To view your own database in your Jupyter Notebook, create the following bash file in your project directory and run with ./start.sh . Access the web server using the base URL with the port number you are using in the below file. start.sh #!/bin/bash # This script just starts Datasette with the correct URL, so # that you can use it within kubeflow. # Get an example database wget https://github.com/StatCan/R-notebooks/raw/master/database-connections/latin_phrases.db # If you have your own database, you can change this line! DATABASE = latin_phrases.db export BASE_URL = \"https://kubeflow.covid.cloud.statcan.ca ${ JUPYTER_SERVER_URL : 19 } proxy/8001/\" echo \"Base url: ${ BASE_URL } \" datasette $DATABASE --cors --config max_returned_rows:100000 --config sql_time_limit_ms:5500 --config base_url: ${ BASE_URL } Check out this video tutorial One user of the platform used Datasette along with a javascript dashboard. See it video for a Running your Notebook Server and accessing the port When running any tool from your Jupyter Notebook that posts a website to a port, you will not be able to simply access it from http://localhost:5000/ as normally suggested in the output upon running the web-app. To access the web server you will need to use the base URL. In your notebook terminal run: python echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/","title":"Starting Datasette"},{"location":"2-Publishing/PowerBI/","text":"Loading data into Power BI \u00b6 We do not offer a Power BI server, but you can pull your data into Power BI from our Storage system, and use the data as a pandas data frame. What you'll need \u00b6 A computer with Power BI, and Python 3.6 Your MinIO ACCESS_KEY and SECRET_KEY on hand. (See Storage ) Get connected \u00b6 Set up Power BI \u00b6 Open up your Power BI system, and open up this Power BI quick start in your favourite text editor. You'll have to make sure that pandas , boto3 , and numpy are installed, and that you're using the right Conda virtual environment (if applicable). You'll then need to make sure that Power BI is using the correct Python environment. This is modified from the options menu, and the exact path is specified in the quick start guide. Edit your python script \u00b6 Then, edit your Python script to use your MinIO ACCESS_KEY and SECRET_KEY , and then click \"Get Data\" and copy it in as a Python Script.","title":"Power BI"},{"location":"2-Publishing/PowerBI/#loading-data-into-power-bi","text":"We do not offer a Power BI server, but you can pull your data into Power BI from our Storage system, and use the data as a pandas data frame.","title":"Loading data into Power BI"},{"location":"2-Publishing/PowerBI/#what-youll-need","text":"A computer with Power BI, and Python 3.6 Your MinIO ACCESS_KEY and SECRET_KEY on hand. (See Storage )","title":"What you'll need"},{"location":"2-Publishing/PowerBI/#get-connected","text":"","title":"Get connected"},{"location":"2-Publishing/PowerBI/#set-up-power-bi","text":"Open up your Power BI system, and open up this Power BI quick start in your favourite text editor. You'll have to make sure that pandas , boto3 , and numpy are installed, and that you're using the right Conda virtual environment (if applicable). You'll then need to make sure that Power BI is using the correct Python environment. This is modified from the options menu, and the exact path is specified in the quick start guide.","title":"Set up Power BI"},{"location":"2-Publishing/PowerBI/#edit-your-python-script","text":"Then, edit your Python script to use your MinIO ACCESS_KEY and SECRET_KEY , and then click \"Get Data\" and copy it in as a Python Script.","title":"Edit your python script"},{"location":"2-Publishing/R-Shiny/","text":"Deploying your R-Shiny dashboard! \u00b6 We handle the R-Shiny server, and it's super easy to get your dashboard onto the platform. Just send a pull request! \u00b6 All you have to do is send a pull request to our R-Dashboards repository . Include your repository in a folder with the name you want (for example, \"air-quality-dashboard\"). Then we will approve it and it will come online. If you need extra R libraries to be installed, send your list to the R-Shiny repository by creating a GitHub Issue and we will add the dependencies. See the above dashboard here The above dashboard is in GitHub. Take a look at the source , and see the dashboard live . Embedding dashboards into your websites \u00b6 Embedding dashboards in other sites We have not had a chance to look at this or prototype it yet, but if you have a use-case, feel free to reach out to engineering. We will work with you to figure something out.","title":"R Shiny"},{"location":"2-Publishing/R-Shiny/#deploying-your-r-shiny-dashboard","text":"We handle the R-Shiny server, and it's super easy to get your dashboard onto the platform.","title":"Deploying your R-Shiny dashboard!"},{"location":"2-Publishing/R-Shiny/#just-send-a-pull-request","text":"All you have to do is send a pull request to our R-Dashboards repository . Include your repository in a folder with the name you want (for example, \"air-quality-dashboard\"). Then we will approve it and it will come online. If you need extra R libraries to be installed, send your list to the R-Shiny repository by creating a GitHub Issue and we will add the dependencies. See the above dashboard here The above dashboard is in GitHub. Take a look at the source , and see the dashboard live .","title":"Just send a pull request!"},{"location":"2-Publishing/R-Shiny/#embedding-dashboards-into-your-websites","text":"Embedding dashboards in other sites We have not had a chance to look at this or prototype it yet, but if you have a use-case, feel free to reach out to engineering. We will work with you to figure something out.","title":"Embedding dashboards into your websites"},{"location":"3-Pipelines/Kubeflow-Pipelines/","text":"Overview \u00b6 Kubeflow Pipelines is a platform for building machine learning workflows for deployment in a Kubernetes environment. It enables authoring pipelines that encapsulate analytical workflows (transforming data, training models, building visuals, etc.). These pipelines can be shared, reused, and scheduled, and are built to run on compute provided via Kubernetes. Here is an example of a pipeline with many sample steps feeding into a single average step. This image comes from the Kubeflow Pipelines UI In the context of the Advanced Analytics Workspace, Kubeflow Pipelines are interacted with through: The Kubeflow UI , where from the Pipelines menu you can upload pipelines, view the pipelines you have and their results, etc. The Kubeflow Pipelines python SDK , accessible through the Jupyter Notebook Servers , where you can define your components and pipelines, submit them to run now, or even save them for later. More examples in the notebooks More comprehensive pipeline examples specifically made for this platform are available on GitHub (and in every Notebook Server at /jupyter-notebooks ). You can also check out public sources . See the official Kubeflow docs for a more detailed explanation of Kubeflow Pipelines. What are pipelines and how do they work? \u00b6 A pipeline in Kubeflow Pipelines consists of one or more pipeline components chained together to form a workflow. The components are like functions, describing the individual steps in your workflow (such as pulling columns from a data store, transforming data, or training a model). The pipeline is the logic that glues components together, such as: Run Component-A Pass the output from Component-A to Component-B and Component-C ... In the above image, the logic would be running many sample steps followed by a single average step. At their core, each component has: A standalone application, packaged as a Docker image , for doing the actual work. The code in the Docker image could be a shell script, Python script, or anything else you can run from a Linux terminal, and generally will have a command line interface for data exchange (accessible through docker run ) A YAML file that describes how Kubeflow Pipelines runs this code (what Docker image should be run, what command line arguments does it accept, what output does it generate) Each component should be single purpose , modular , and reusable . Define and run your first pipeline using the Python SDK \u00b6 While pipelines and components are defined in Kubeflow Pipelines by YAML files that use Docker images, that does not mean we have to work directly with either YAML files or Docker images. The Kubeflow Pipelines SDK provides a way for us to define our pipeline and components directly in Python code, where the SDK then translates our Python code to YAML files for us. For our first example, let's define a simple pipeline using only the Python SDK. The purpose of this section is to give a high level view of component and pipeline authoring, not a deep dive. More detailed looks into defining your own components , passing data between components , and returning data from your pipeline are explained in more detail in further sections. The demo pipeline we define will do the following: Accept five numbers as arguments Average of the first three numbers Average of the last two numbers Average of the results of (2) and (3) To do this, we will first define our component . Our average component will call a Docker image that does the following: Accepts one or more numbers as command line arguments Returns the average of these numbers by writing them to an output file in the container (by default, to out.txt ) This Docker image is already built for us and stored in our container registry here: k8scc01covidacr.azurecr.io/kfp-components/average:v1 . Don't worry if you don't know Docker - since the image is built already, we only have to tell Kubeflow Pipelines where it is. ??? info \"Full details of the average component's Docker image are in GitHub \" This image effectively runs the following code (slightly cleaned up for brevity). By making average.py accept an arbitrary set of numbers as inputs, we can use the same average component for all steps in our pipeline : import argparse def parse_args (): parser = argparse . ArgumentParser ( description = \"Returns the average of one or \" \"more numbers as a JSON file\" ) parser . add_argument ( \"numbers\" , type = float , nargs = \"+\" , help = \"One or more numbers\" ) parser . add_argument ( \"--output_file\" , type = str , default = \"out.txt\" , help = \"Filename \" \"to write output number to\" ) return parser . parse_args () if __name__ == '__main__' : args = parse_args () numbers = args . numbers output_file = args . output_file print ( f \"Averaging numbers: { numbers } \" ) avg = sum ( numbers ) / len ( numbers ) print ( f \"Result = { avg } \" ) print ( f \"Writing output to { output_file } \" ) with open ( output_file , 'w' ) as fout : fout . write ( str ( avg )) print ( \"Done\" ) To make our average image into a Kubeflow Pipelines component , we make a kfp.dsl.ContainerOp in Python that defines how Kubeflow Pipelines interacts with our container, specifying: The Docker image location to use How to pass arguments to the running container What outputs to expect from the container We could use ContainerOp directly, but since we'll use average a few times we instead create a factory function we can reuse: from kfp import dsl def average_op ( * numbers ): \"\"\" Factory for average ContainerOps Accepts an arbitrary number of input numbers, returning a ContainerOp that passes those numbers to the underlying Docker image for averaging Returns output collected from ./out.txt from inside the container \"\"\" # Input validation if len ( numbers ) < 1 : raise ValueError ( \"Must specify at least one number to take the average of\" ) return dsl . ContainerOp ( name = \"average\" , # What will show up on the pipeline viewer image = \"k8scc01covidacr.azurecr.io/kfp-components/average:v1\" , # The image that KFP runs to do the work arguments = numbers , # Passes each number as a separate command line argument # Note that these arguments get serialized to strings file_outputs = { 'data' : './out.txt' }, # Expect an output file called out.txt to be generated # KFP can read this file and bring it back automatically ) To define our pipeline, we create a Python function decorated by the @dsl.pipeline decorator. We invoke our average_op factory to use our average container. We pass each average some inputs, and even use their outputs by accessing avg_*.output . @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) Finally, while we've defined our pipeline in Python, Kubeflow Pipelines itself needs everything defined as a YAML file. This final step uses the Kubeflow Pipelines Python SDK to translate our pipeline function into a YAML file that describes exactly how Kubeflow Pipelines can interact with our component. Unzip it and take a look for yourself! from kfp import compiler pipeline_yaml = 'pipeline.yaml.zip' compiler . Compiler () . compile ( my_pipeline , pipeline_yaml ) print ( f \"Exported pipeline definition to { pipeline_yaml } \" ) Kubeflow Pipelines is a lazy beast It is useful to keep in mind what computation is happening when you run this python code versus what happens when you submit the pipeline to Kubeflow Pipelines. Although it seems like everything is happening in the moment, try adding print(avg_1.output) to the above pipeline and see what happens when you compile your pipeline. The Python SDK we're using is for authoring pipelines, not for running them, so results from components will never be available when you run this Python code. The is discussed more below in Understanding what computation occurs when . To actually run our pipeline, we define an experiment: experiment_name = \"averaging-pipeline\" import kfp client = kfp . Client () exp = client . create_experiment ( name = experiment_name ) pl_params = { 'a' : 5 , 'b' : 5 , 'c' : 8 , 'd' : 10 , 'e' : 18 , } And then run an instance of our pipeline with the arguments we want: import time run = client . run_pipeline ( exp . id , # Run inside the above experiment experiment_name + '-' + time . strftime ( \"%Y%m %d -%H%M%S\" ), # Give our job a name with a timestamp so its unique pipeline_yaml , # Pass the .yaml.zip we created above. This defines the pipeline params = pl_params # Pass our parameters we want to run the pipeline with ) This can all be seen in the Kubeflow Pipelines UI : Later when we want to reuse the pipeline, we can pass different arguments and do it all again. !!! info \"We create our experiment, upload our pipeline, and run from Python in this example, but we could also do all this through the Kubeflow Pipelines UI above. Understanding what computation occurs when \u00b6 The above example uses Python code to define: The interface between Kubeflow Pipelines and our Docker containers doing the work (by defining ContainerOp 's) The logic of our pipeline (by defining my_pipeline ). But when we run compiler.Compiler().compile() and client.run_pipeline() , what actually happens? It is important to remember that everything we run in Python here is specifying the pipeline and its components in order to write YAML definitions, it is not doing the work of the pipeline. When running compiler.Compiler().compile() , we are not running our pipeline in the typical sense. Instead, KFP uses my_pipeline to build a YAML version of it. When we compile , the KFP SDK is passing placeholder arguments to my_pipeline and tracing where they (and any other runtime data) go, such as any output a component produces. When compile encounters a ContainerOp , nothing runs now - instead it takes note that a container will be there in future and remembers what data it will consume/generate. This can be seen by modifying and recompiling our pipeline: @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # NEW CODE x = 1 + 1 print ( f \"The value of x is { x } \" ) print ( f \"The value of a is { a } \" ) # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # NEW CODE print ( f \"The value of avg_1.output is { avg_1 . output } \" ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) And when we compile , we see print statements: The value of x is 2 The value of a is {{ pipelineparam : op = ; name = a }} The value of avg_1.output is {{ pipelineparam : op = averge ; name = data }} In the first print statement everything is normal. In the second and third print statements, however, we see string placeholders rather than actual output. So while compile does \"execute\" my_pipeline , the KFP-specific parts of the code don't actually generate results. This can also be seen in the YAML file that compile generates, for example looking at the portion defining our average_result_overall component: - name : average-3 container : args : [ \"{{inputs.parameters.average-data}}\" , \"{{inputs.parameters.average-2-data}}\" , ] image : k8scc01covidacr.azurecr.io/kfp-components/average:v1 inputs : parameters : - { name : average-2-data } - { name : average-data } outputs : artifacts : - { name : average-3-data , path : ./out.txt } metadata : labels : { pipelines.kubeflow.org/pipeline-sdk-type : kfp } In this YAML we see the input parameters passed are placeholders for data from previous components rather than their actual value. This is because while KFP knows a result from average-data and average-2-data will be passed to average, but the value of that result is not available until the pipeline is actually run. Component naming within the YAML file Because we made an average_op factory function with name='average' above, our YAML file has component names that automatically increment to avoid recreating the same name twice. We could have been fancier with our factory functions to more directly control our names, giving an argument like name='average_first_input_args' , or could even have explicitly defined the name in our pipeline by using avg_1 = average_op(a, b, c).set_display_name(\"Average 1\") . As one more example, let's try two more pipelines. One has a for loop inside which prints \"Woohoo!\" a fixed number of times. whereas the other does the same but loops n times (where n is a pipeline parameter): !!! info \"Pipeline parameters are described more below, but they work like parameters for functions. Pipelines can accept data (numbers, string URL's to large files in MinIO, etc.) as arguments, allowing a single generic pipeline to work in many situations.\" @dsl . pipeline ( name = \"my pipeline's name\" ) def another_pipeline (): \"\"\" Prints to the screen 10 times \"\"\" for i in range ( 10 ): print ( \"Woohoo!\" ) # And just so we've got some component going too... avg = average_op ( n ) compiler . Compiler () . compile ( another_pipeline , \"another.yaml.zip\" ) @dsl . pipeline ( name = \"my pipeline's name\" ) def another_another_pipeline ( n ): \"\"\" Prints to the screen n times \"\"\" for i in range ( n ): print ( \"Woohoo!\" ) # And just so we've got some component going too... avg = average_op ( n ) compiler . Compiler () . compile ( another_another_pipeline , \"another.yaml.zip\" ) The first works as you'd expect, but the second raises the exception: TypeError: 'PipelineParam' object cannot be interpreted as an integer Why? Because when authoring the pipeline n is a placeholder and has no value. KFP cannot define a pipeline from this because it does not know how many times to loop. We'd hit similar problems if using if statements. There are some ways around this, but they're left to the reader to explore through the Kubeflow Pipelines docs . Why does pipeline authoring behave this way? Because pipelines (and components ) are meant to be reusable definitions of logic that are defined in static YAML files, with all dynamic decision making done inside components. This can make them a little awkward to define, but also helps them be more reusable. Data Exchange \u00b6 Passing data into, within, and from a pipeline \u00b6 In the first example above, we pass: Numbers into our pipeline Numbers between components within our pipeline A number back to the user at the end But as discussed above, pipeline arguments and component results are just placeholder objects \u2013 so how does KFP know our values are numeric? The answer is: it doesn't. In fact, it didn't even treat them as numbers above. Instead it treated them as strings. It is just that our pipeline components worked just as well with \"5\" as they would have with 5 . A safe default assumption is that all data exchange happens as a string. When we passed a, b, ... into the pipeline, those numbers were implicitly stringified because they eventually become command line arguments for our Docker container. When we read the result of avg_1 from its out.txt , that result was read as a string. By calling average_op(avg_1.output, avg_2.output) , we ask KFP to pass the string output from avg_1 and avg_2 to a new average_op . It just so happened that, since average_op passes each string as a command line argument to our Docker image, it didn't really matter they were strings. You can still use non-string data types, but you need to pass them as serialized versions. So if we wanted our avg_1 component to return both the numbers passed to it and the average returned as a dictionary, for example: { 'numbers' : [ 5 , 5 , 8 ], 'result' : 6.0 , } We could modify our average.py in the Docker image write our dictionary of numbers and result to out.txt as JSON. But then when we pass the result to make average_result_overall , that component needs to deserialize the above JSON and pull the data from it that it needs. And because these results are not available when authoring the pipeline, something like this does not work: def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op_that_returns_json ( a , b , c ) avg_2 = average_op_that_returns_json ( d , e ) # THIS DOES NOT WORK! import json avg_1_result = json . loads ( avg_1 . output )[ 'result' ] avg_2_result = json . loads ( avg_2 . output )[ 'result' ] # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) At compile time, avg_1.output is just a placeholder and can't be treated like the JSON it will eventually become. To do something like this, we need to interpret the JSON string within a container. Passing Secrets \u00b6 Pipelines often need sensitive information (passwords, API keys, etc.) to operate. To keep these secure, these cannot be passed to a Kubeflow Pipeline using a pipeline argument, environment variable, or as a hard coded value in python code, as they will be exposed as plain text for others to read. To address this issue, we use Kubernetes secrets as a way to securely store and pass sensitive information. Each secret is a key-value store containing some number of key-value pairs. The secrets can be passed by reference to the pipeline. Secrets are only accessible within their own namespace Secrets can only be accessed within the namespace they are created in. If you need to use a secret in another namespace, you need to add it there manually. Create a key-value store \u00b6 The example below creates a key-value store called elastic-credentials which contains two key-value pairs: \"username\": \"USERNAME\", \"password\": \"PASSWORD\" kubectl create secret generic elastic-credentials \\ --from-literal = username = \"YOUR_USERNAME\" \\ --from-literal = password = \"YOUR_PASSWORD\" Get an existing key-value store \u00b6 # The secrets will be base64 encoded. kubectl get secret elastic-credentials Mounting Kubernetes Secrets to Environment Variables in Container Operations \u00b6 Once the secrets are defined in the project namespace, you can mount specific secrets as environment variables in your container using the Kubeflow SDK. Example This example is based off of a snippet from the Python KFP source code . This example shows how (1) an Elasticsearch username(2) an Elasticsearch password, and (3) a GitLab deploy token are passed to the container operation as environment variables. # Names of k8s secret key-value stores ES_CREDENTIALS_STORE = \"elastic-credentials\" GITLAB_CREDENTIALS_STORE = \"gitlab-credentials\" # k8s secrets key names ES_USER_KEY = \"username\" ES_PASSWORD_KEY = \"password\" GITLAB_DEPLOY_TOKEN_KEY = 'token' # Names of environment variables that secrets should be mounted to in the # container ES_USER_ENV = \"ES_USER\" ES_PASS_ENV = \"ES_PASS\" GITLAB_DEPLOY_TOKEN_ENV = \"GITLAB_DEPLOY_TOKEN\" # ... container_operation = dsl . ContainerOp ( name = 'some-op' , image = 'some-image' , ) . add_env_variable ( k8s_client . V1EnvVar ( name = ES_USER_ENV , value_from = k8s_client . V1EnvVarSource ( secret_key_ref = k8s_client . V1SecretKeySelector ( name = ES_CREDENTIALS_STORE , key = ES_USER_KEY ) ) ) ) \\ . add_env_variable ( k8s_client . V1EnvVar ( name = ES_PASS_ENV , value_from = k8s_client . V1EnvVarSource ( secret_key_ref = k8s_client . V1SecretKeySelector ( name = ES_CREDENTIALS_STORE , key = ES_PASSWORD_KEY ) ) ) ) \\ . add_env_variable ( k8s_client . V1EnvVar ( name = GITLAB_DEPLOY_TOKEN_ENV , value_from = k8s_client . V1EnvVarSource ( secret_key_ref = k8s_client . V1SecretKeySelector ( name = GITLAB_CREDENTIALS_STORE , key = GITLAB_DEPLOY_TOKEN_KEY ) ) ) ) Parameterizing pipelines \u00b6 Whenever possible, create pipelines in a generic way: define parameters that might change as pipeline inputs instead of writing values directly in your Python code. For example, if you want a pipeline to process data from minimal-tenant/john-smith/data1.csv , don't hard code that path - instead, accept it as a pipeline parameter. This way you can call the same pipeline repeatedly by passing it the data location as an argument. You can see this approach in our example notebooks , where we accept MinIO credentials and the location to store our results as pipeline parameters. Passing complex/large data to/from a pipeline \u00b6 Although small data can often be stringified, passing by string is not suitable for complex data (large parquet files, images, etc.). It is common to use blob storage (for example: MinIO ) or other outside storage methods to persist data between components or even for later use. A typical pattern would be: Upload large/complex input data to blob storage (e.g. training data, a saved model, etc.) Pass the location of this data into the pipeline as parameters, and make your pipeline/components fetch the data as required For each component in a pipeline, specify where they place outputs in the same way For each component also return the path where it has stored its data (in this case, the string we passed it in the above bullet). This feels redundant, but it is a common pattern that lets you chain operations together Here is a schematic example of this pattern: def my_blobby_pipeline ( path_to_numbers_1 , path_to_numbers_2 , path_for_output ): \"\"\" Averaging pipeline which accepts two groups of numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op_that_takes_path_to_blob ( path_to_numbers = path_to_numbers_1 , output_location = path_for_output + \"/avg_1\" ) avg_2 = average_op_that_takes_path_to_blob ( numbers = path_to_numbers_2 , output_location = path_for_output + \"/avg_2\" ) # Note that this assumes the average_op can take multiple paths to numbers. You could also have an # aggregation component that combines avg_1 and avg_2 into a single file of numbers paths_to_numbers = [ avg_1 . output , avg_2 . output ] average_result_overall = average_op ( path_to_numbers = paths_to_numbers , output_location = path_for_output + \"/average_result_overall\" ) Within this platform, the primary method for persisting large files is through MinIO as described in our Storage documentation . Examples of this are also described in our example notebooks (also found in jupyter-notebooks/self-serve-storage/ on any notebook server). Typical development patterns \u00b6 End-to-end pipeline development \u00b6 A typical pattern for building pipelines in Kubeflow Pipelines is: Define components for each of your tasks Compose your components in a @dsl.pipeline decorated function compile() your pipeline, upload your YAML files, and run This pattern lets you define portable components that can be individually tested before combining them into a full pipeline. Depending on the type and complexity of task, there are different methods for building the components. Methods for authoring components \u00b6 Fundamentally, every component in Kubeflow Pipelines runs a container. Kubeflow Pipelines offers several methods to define these components with different levels of flexibility and complexity. User-defined container components \u00b6 You can define tasks through custom Docker images. The design pattern for this is: Define (update) code for your task and commit to Git Build an image from your task (through manual command or CI pipeline) Test running this Docker image locally (and iterate if needed) Push the image to a container registry (usually Docker hub, but it will be Azure Container Registry in our case on the Advanced Analytics Workspace) Update the Kubeflow Pipeline to point to the new image (via dsl.ContainerOp like above) and test the pipeline This lets you run anything you can put into a Docker image as a task in Kubeflow Pipelines. You can manage and test your images and have complete control over how they run and what dependencies use. The docker run interface for each container becomes the API that Kubeflow Pipelines dsl.ContainerOp interacts with \u2013 running the containers is effectively like running them locally using a terminal. Anything you can make into a container with that interface can be run in Kubeflow Pipelines. !!! danger \"...however, for security reasons the platform currently does not allow users to build/run custom Docker images. This is planned for the future, but in interim see Lightweight components for a way to develop pipelines without custom images\" Lightweight Python components \u00b6 While full custom containers offer great flexibility, sometimes they're heavier than needed. The Kubeflow Pipelines SDK also allows for Lightweight Python Components , which are components that can be built straight from Python without building new container images for each change. These components are great for fast iteration during development, as well as for simple tasks that can be written and managed easily. This is an example of a lightweight pipeline with a single component that concatenates strings: import kfp from kfp import dsl from kfp.components import func_to_container_op def concat_string ( a , b ) -> str : return f \"( { a } | { b } )\" concat_string_component = func_to_container_op ( concat_string , base_image = \"python:3.8.3-buster\" ) @dsl . pipeline ( name = \"My lightweight pipeline\" , ) def pipeline ( str1 , str2 , str3 ): # Note that we use the concat_string_component, not the # original concat_string() function concat_result_1 = concat_string_component ( str1 , str2 ) # By using cancat_result_1's output, we define the dependency of # concat_result_2 on concat_result_1 concat_result_2 = concat_string_component ( concat_result_1 . output , str3 ) We see that our concat_string component is defined directly in Python rather than from a Docker image. In the end, our function still runs in a container, but we don't have to built it ourselves: func_to_container_op() runs our Python code inside the provided base image ( python:3.8.3-buster ). This lets use avoid building every time we change our code. The base image can be anything accessible by Kubeflow, which includes all images in the Azure Container Registry and any whitelisted images from Docker hub. Lightweight components have a number of advantages but also some drawbacks See this description of their basic characteristics, as well as this example which uses them in a more complex pipeline A convenient base image to use is the the image your notebook server is running By using the same image as your notebook server, you ensure Kubeflow Pipelines has the same packages available to it as the notebook where you do your analysis. This can help avoid errors from importing packages specific to your environment. You can find that link from the notebook server page as shown below, but make sure you prepend the registry URL (so the below image would have base_image=k8scc01covidacr.azurecr.io/machine-learning-notebook-cpu:562fa4a2899eeb9ae345c51c2491447ec31a87d7 ). Note that while using a fully featured base image for iteration is fine, it's good practice to keep production pipelines lean and only supply the necessary software. That way you reduce the startup time for each step in your pipeline. Defining components directly in YAML \u00b6 Components can be defined directly with a YAML file, where the designer can run terminal commands from a given Docker image. This can be a great way to make non-Python pipeline components from existing containers. As with all components, we can pass both arguments and data files into/out of the component. For example: name : Concat Strings inputs : - { name : Input text 1 , type : String , description : \"Some text to echo into a terminal and tee to a file\" , } - { name : Input text 2 , type : String , description : \"Some text to echo into a terminal and tee to a file\" , } outputs : - { name : Output filename , type : String } implementation : container : image : bash:5 command : - bash - -ex - -c - | echo \"$0 | $1\" | tee $2 - { inputValue : Input text 1 } - { inputValue : Input text 2 } - { outputPath : Output filename } This example concatenates two strings like our lightweight example above. We then define a component in python from this YAML: from kfp.components import load_component_from_file echo_and_tee = load_component_from_file ( 'path/to/echo_and_tee.yaml' ) @dsl . pipeline def my_pipeline (): echo_and_tee_task_1 = echo_and_tee ( \"My text to echo\" ) # A second use that consumes the return of the first one echo_and_tee_task_2 = echo_and_tee ( echo_and_tee_task_1 . output ) See this example for more details on using existing components. Reusing existing components \u00b6 Similar to well abstracted functions, well abstracted components can reduce the amount of code you have to write for any given project. For example, rather than teaching your machine learning train_model component to also save the resulting model to MinIO, you can instead have train_model return the model and then Kubeflow Pipelines can pass the model to a reusable copy_to_minio component. This reuse pattern applies to components defined through any means (containers, lightweight, or YAML). Take a look at our example notebook , which reuses provided components for simple file IO tasks.","title":"Kubeflow Pipelines"},{"location":"3-Pipelines/Kubeflow-Pipelines/#overview","text":"Kubeflow Pipelines is a platform for building machine learning workflows for deployment in a Kubernetes environment. It enables authoring pipelines that encapsulate analytical workflows (transforming data, training models, building visuals, etc.). These pipelines can be shared, reused, and scheduled, and are built to run on compute provided via Kubernetes. Here is an example of a pipeline with many sample steps feeding into a single average step. This image comes from the Kubeflow Pipelines UI In the context of the Advanced Analytics Workspace, Kubeflow Pipelines are interacted with through: The Kubeflow UI , where from the Pipelines menu you can upload pipelines, view the pipelines you have and their results, etc. The Kubeflow Pipelines python SDK , accessible through the Jupyter Notebook Servers , where you can define your components and pipelines, submit them to run now, or even save them for later. More examples in the notebooks More comprehensive pipeline examples specifically made for this platform are available on GitHub (and in every Notebook Server at /jupyter-notebooks ). You can also check out public sources . See the official Kubeflow docs for a more detailed explanation of Kubeflow Pipelines.","title":"Overview"},{"location":"3-Pipelines/Kubeflow-Pipelines/#what-are-pipelines-and-how-do-they-work","text":"A pipeline in Kubeflow Pipelines consists of one or more pipeline components chained together to form a workflow. The components are like functions, describing the individual steps in your workflow (such as pulling columns from a data store, transforming data, or training a model). The pipeline is the logic that glues components together, such as: Run Component-A Pass the output from Component-A to Component-B and Component-C ... In the above image, the logic would be running many sample steps followed by a single average step. At their core, each component has: A standalone application, packaged as a Docker image , for doing the actual work. The code in the Docker image could be a shell script, Python script, or anything else you can run from a Linux terminal, and generally will have a command line interface for data exchange (accessible through docker run ) A YAML file that describes how Kubeflow Pipelines runs this code (what Docker image should be run, what command line arguments does it accept, what output does it generate) Each component should be single purpose , modular , and reusable .","title":"What are pipelines and how do they work?"},{"location":"3-Pipelines/Kubeflow-Pipelines/#define-and-run-your-first-pipeline-using-the-python-sdk","text":"While pipelines and components are defined in Kubeflow Pipelines by YAML files that use Docker images, that does not mean we have to work directly with either YAML files or Docker images. The Kubeflow Pipelines SDK provides a way for us to define our pipeline and components directly in Python code, where the SDK then translates our Python code to YAML files for us. For our first example, let's define a simple pipeline using only the Python SDK. The purpose of this section is to give a high level view of component and pipeline authoring, not a deep dive. More detailed looks into defining your own components , passing data between components , and returning data from your pipeline are explained in more detail in further sections. The demo pipeline we define will do the following: Accept five numbers as arguments Average of the first three numbers Average of the last two numbers Average of the results of (2) and (3) To do this, we will first define our component . Our average component will call a Docker image that does the following: Accepts one or more numbers as command line arguments Returns the average of these numbers by writing them to an output file in the container (by default, to out.txt ) This Docker image is already built for us and stored in our container registry here: k8scc01covidacr.azurecr.io/kfp-components/average:v1 . Don't worry if you don't know Docker - since the image is built already, we only have to tell Kubeflow Pipelines where it is. ??? info \"Full details of the average component's Docker image are in GitHub \" This image effectively runs the following code (slightly cleaned up for brevity). By making average.py accept an arbitrary set of numbers as inputs, we can use the same average component for all steps in our pipeline : import argparse def parse_args (): parser = argparse . ArgumentParser ( description = \"Returns the average of one or \" \"more numbers as a JSON file\" ) parser . add_argument ( \"numbers\" , type = float , nargs = \"+\" , help = \"One or more numbers\" ) parser . add_argument ( \"--output_file\" , type = str , default = \"out.txt\" , help = \"Filename \" \"to write output number to\" ) return parser . parse_args () if __name__ == '__main__' : args = parse_args () numbers = args . numbers output_file = args . output_file print ( f \"Averaging numbers: { numbers } \" ) avg = sum ( numbers ) / len ( numbers ) print ( f \"Result = { avg } \" ) print ( f \"Writing output to { output_file } \" ) with open ( output_file , 'w' ) as fout : fout . write ( str ( avg )) print ( \"Done\" ) To make our average image into a Kubeflow Pipelines component , we make a kfp.dsl.ContainerOp in Python that defines how Kubeflow Pipelines interacts with our container, specifying: The Docker image location to use How to pass arguments to the running container What outputs to expect from the container We could use ContainerOp directly, but since we'll use average a few times we instead create a factory function we can reuse: from kfp import dsl def average_op ( * numbers ): \"\"\" Factory for average ContainerOps Accepts an arbitrary number of input numbers, returning a ContainerOp that passes those numbers to the underlying Docker image for averaging Returns output collected from ./out.txt from inside the container \"\"\" # Input validation if len ( numbers ) < 1 : raise ValueError ( \"Must specify at least one number to take the average of\" ) return dsl . ContainerOp ( name = \"average\" , # What will show up on the pipeline viewer image = \"k8scc01covidacr.azurecr.io/kfp-components/average:v1\" , # The image that KFP runs to do the work arguments = numbers , # Passes each number as a separate command line argument # Note that these arguments get serialized to strings file_outputs = { 'data' : './out.txt' }, # Expect an output file called out.txt to be generated # KFP can read this file and bring it back automatically ) To define our pipeline, we create a Python function decorated by the @dsl.pipeline decorator. We invoke our average_op factory to use our average container. We pass each average some inputs, and even use their outputs by accessing avg_*.output . @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) Finally, while we've defined our pipeline in Python, Kubeflow Pipelines itself needs everything defined as a YAML file. This final step uses the Kubeflow Pipelines Python SDK to translate our pipeline function into a YAML file that describes exactly how Kubeflow Pipelines can interact with our component. Unzip it and take a look for yourself! from kfp import compiler pipeline_yaml = 'pipeline.yaml.zip' compiler . Compiler () . compile ( my_pipeline , pipeline_yaml ) print ( f \"Exported pipeline definition to { pipeline_yaml } \" ) Kubeflow Pipelines is a lazy beast It is useful to keep in mind what computation is happening when you run this python code versus what happens when you submit the pipeline to Kubeflow Pipelines. Although it seems like everything is happening in the moment, try adding print(avg_1.output) to the above pipeline and see what happens when you compile your pipeline. The Python SDK we're using is for authoring pipelines, not for running them, so results from components will never be available when you run this Python code. The is discussed more below in Understanding what computation occurs when . To actually run our pipeline, we define an experiment: experiment_name = \"averaging-pipeline\" import kfp client = kfp . Client () exp = client . create_experiment ( name = experiment_name ) pl_params = { 'a' : 5 , 'b' : 5 , 'c' : 8 , 'd' : 10 , 'e' : 18 , } And then run an instance of our pipeline with the arguments we want: import time run = client . run_pipeline ( exp . id , # Run inside the above experiment experiment_name + '-' + time . strftime ( \"%Y%m %d -%H%M%S\" ), # Give our job a name with a timestamp so its unique pipeline_yaml , # Pass the .yaml.zip we created above. This defines the pipeline params = pl_params # Pass our parameters we want to run the pipeline with ) This can all be seen in the Kubeflow Pipelines UI : Later when we want to reuse the pipeline, we can pass different arguments and do it all again. !!! info \"We create our experiment, upload our pipeline, and run from Python in this example, but we could also do all this through the Kubeflow Pipelines UI above.","title":"Define and run your first pipeline using the Python SDK"},{"location":"3-Pipelines/Kubeflow-Pipelines/#understanding-what-computation-occurs-when","text":"The above example uses Python code to define: The interface between Kubeflow Pipelines and our Docker containers doing the work (by defining ContainerOp 's) The logic of our pipeline (by defining my_pipeline ). But when we run compiler.Compiler().compile() and client.run_pipeline() , what actually happens? It is important to remember that everything we run in Python here is specifying the pipeline and its components in order to write YAML definitions, it is not doing the work of the pipeline. When running compiler.Compiler().compile() , we are not running our pipeline in the typical sense. Instead, KFP uses my_pipeline to build a YAML version of it. When we compile , the KFP SDK is passing placeholder arguments to my_pipeline and tracing where they (and any other runtime data) go, such as any output a component produces. When compile encounters a ContainerOp , nothing runs now - instead it takes note that a container will be there in future and remembers what data it will consume/generate. This can be seen by modifying and recompiling our pipeline: @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # NEW CODE x = 1 + 1 print ( f \"The value of x is { x } \" ) print ( f \"The value of a is { a } \" ) # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # NEW CODE print ( f \"The value of avg_1.output is { avg_1 . output } \" ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) And when we compile , we see print statements: The value of x is 2 The value of a is {{ pipelineparam : op = ; name = a }} The value of avg_1.output is {{ pipelineparam : op = averge ; name = data }} In the first print statement everything is normal. In the second and third print statements, however, we see string placeholders rather than actual output. So while compile does \"execute\" my_pipeline , the KFP-specific parts of the code don't actually generate results. This can also be seen in the YAML file that compile generates, for example looking at the portion defining our average_result_overall component: - name : average-3 container : args : [ \"{{inputs.parameters.average-data}}\" , \"{{inputs.parameters.average-2-data}}\" , ] image : k8scc01covidacr.azurecr.io/kfp-components/average:v1 inputs : parameters : - { name : average-2-data } - { name : average-data } outputs : artifacts : - { name : average-3-data , path : ./out.txt } metadata : labels : { pipelines.kubeflow.org/pipeline-sdk-type : kfp } In this YAML we see the input parameters passed are placeholders for data from previous components rather than their actual value. This is because while KFP knows a result from average-data and average-2-data will be passed to average, but the value of that result is not available until the pipeline is actually run. Component naming within the YAML file Because we made an average_op factory function with name='average' above, our YAML file has component names that automatically increment to avoid recreating the same name twice. We could have been fancier with our factory functions to more directly control our names, giving an argument like name='average_first_input_args' , or could even have explicitly defined the name in our pipeline by using avg_1 = average_op(a, b, c).set_display_name(\"Average 1\") . As one more example, let's try two more pipelines. One has a for loop inside which prints \"Woohoo!\" a fixed number of times. whereas the other does the same but loops n times (where n is a pipeline parameter): !!! info \"Pipeline parameters are described more below, but they work like parameters for functions. Pipelines can accept data (numbers, string URL's to large files in MinIO, etc.) as arguments, allowing a single generic pipeline to work in many situations.\" @dsl . pipeline ( name = \"my pipeline's name\" ) def another_pipeline (): \"\"\" Prints to the screen 10 times \"\"\" for i in range ( 10 ): print ( \"Woohoo!\" ) # And just so we've got some component going too... avg = average_op ( n ) compiler . Compiler () . compile ( another_pipeline , \"another.yaml.zip\" ) @dsl . pipeline ( name = \"my pipeline's name\" ) def another_another_pipeline ( n ): \"\"\" Prints to the screen n times \"\"\" for i in range ( n ): print ( \"Woohoo!\" ) # And just so we've got some component going too... avg = average_op ( n ) compiler . Compiler () . compile ( another_another_pipeline , \"another.yaml.zip\" ) The first works as you'd expect, but the second raises the exception: TypeError: 'PipelineParam' object cannot be interpreted as an integer Why? Because when authoring the pipeline n is a placeholder and has no value. KFP cannot define a pipeline from this because it does not know how many times to loop. We'd hit similar problems if using if statements. There are some ways around this, but they're left to the reader to explore through the Kubeflow Pipelines docs . Why does pipeline authoring behave this way? Because pipelines (and components ) are meant to be reusable definitions of logic that are defined in static YAML files, with all dynamic decision making done inside components. This can make them a little awkward to define, but also helps them be more reusable.","title":"Understanding what computation occurs when"},{"location":"3-Pipelines/Kubeflow-Pipelines/#data-exchange","text":"","title":"Data Exchange"},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-data-into-within-and-from-a-pipeline","text":"In the first example above, we pass: Numbers into our pipeline Numbers between components within our pipeline A number back to the user at the end But as discussed above, pipeline arguments and component results are just placeholder objects \u2013 so how does KFP know our values are numeric? The answer is: it doesn't. In fact, it didn't even treat them as numbers above. Instead it treated them as strings. It is just that our pipeline components worked just as well with \"5\" as they would have with 5 . A safe default assumption is that all data exchange happens as a string. When we passed a, b, ... into the pipeline, those numbers were implicitly stringified because they eventually become command line arguments for our Docker container. When we read the result of avg_1 from its out.txt , that result was read as a string. By calling average_op(avg_1.output, avg_2.output) , we ask KFP to pass the string output from avg_1 and avg_2 to a new average_op . It just so happened that, since average_op passes each string as a command line argument to our Docker image, it didn't really matter they were strings. You can still use non-string data types, but you need to pass them as serialized versions. So if we wanted our avg_1 component to return both the numbers passed to it and the average returned as a dictionary, for example: { 'numbers' : [ 5 , 5 , 8 ], 'result' : 6.0 , } We could modify our average.py in the Docker image write our dictionary of numbers and result to out.txt as JSON. But then when we pass the result to make average_result_overall , that component needs to deserialize the above JSON and pull the data from it that it needs. And because these results are not available when authoring the pipeline, something like this does not work: def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op_that_returns_json ( a , b , c ) avg_2 = average_op_that_returns_json ( d , e ) # THIS DOES NOT WORK! import json avg_1_result = json . loads ( avg_1 . output )[ 'result' ] avg_2_result = json . loads ( avg_2 . output )[ 'result' ] # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) At compile time, avg_1.output is just a placeholder and can't be treated like the JSON it will eventually become. To do something like this, we need to interpret the JSON string within a container.","title":"Passing data into, within, and from a pipeline"},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-secrets","text":"Pipelines often need sensitive information (passwords, API keys, etc.) to operate. To keep these secure, these cannot be passed to a Kubeflow Pipeline using a pipeline argument, environment variable, or as a hard coded value in python code, as they will be exposed as plain text for others to read. To address this issue, we use Kubernetes secrets as a way to securely store and pass sensitive information. Each secret is a key-value store containing some number of key-value pairs. The secrets can be passed by reference to the pipeline. Secrets are only accessible within their own namespace Secrets can only be accessed within the namespace they are created in. If you need to use a secret in another namespace, you need to add it there manually.","title":"Passing Secrets"},{"location":"3-Pipelines/Kubeflow-Pipelines/#create-a-key-value-store","text":"The example below creates a key-value store called elastic-credentials which contains two key-value pairs: \"username\": \"USERNAME\", \"password\": \"PASSWORD\" kubectl create secret generic elastic-credentials \\ --from-literal = username = \"YOUR_USERNAME\" \\ --from-literal = password = \"YOUR_PASSWORD\"","title":"Create a key-value store"},{"location":"3-Pipelines/Kubeflow-Pipelines/#get-an-existing-key-value-store","text":"# The secrets will be base64 encoded. kubectl get secret elastic-credentials","title":"Get an existing key-value store"},{"location":"3-Pipelines/Kubeflow-Pipelines/#mounting-kubernetes-secrets-to-environment-variables-in-container-operations","text":"Once the secrets are defined in the project namespace, you can mount specific secrets as environment variables in your container using the Kubeflow SDK. Example This example is based off of a snippet from the Python KFP source code . This example shows how (1) an Elasticsearch username(2) an Elasticsearch password, and (3) a GitLab deploy token are passed to the container operation as environment variables. # Names of k8s secret key-value stores ES_CREDENTIALS_STORE = \"elastic-credentials\" GITLAB_CREDENTIALS_STORE = \"gitlab-credentials\" # k8s secrets key names ES_USER_KEY = \"username\" ES_PASSWORD_KEY = \"password\" GITLAB_DEPLOY_TOKEN_KEY = 'token' # Names of environment variables that secrets should be mounted to in the # container ES_USER_ENV = \"ES_USER\" ES_PASS_ENV = \"ES_PASS\" GITLAB_DEPLOY_TOKEN_ENV = \"GITLAB_DEPLOY_TOKEN\" # ... container_operation = dsl . ContainerOp ( name = 'some-op' , image = 'some-image' , ) . add_env_variable ( k8s_client . V1EnvVar ( name = ES_USER_ENV , value_from = k8s_client . V1EnvVarSource ( secret_key_ref = k8s_client . V1SecretKeySelector ( name = ES_CREDENTIALS_STORE , key = ES_USER_KEY ) ) ) ) \\ . add_env_variable ( k8s_client . V1EnvVar ( name = ES_PASS_ENV , value_from = k8s_client . V1EnvVarSource ( secret_key_ref = k8s_client . V1SecretKeySelector ( name = ES_CREDENTIALS_STORE , key = ES_PASSWORD_KEY ) ) ) ) \\ . add_env_variable ( k8s_client . V1EnvVar ( name = GITLAB_DEPLOY_TOKEN_ENV , value_from = k8s_client . V1EnvVarSource ( secret_key_ref = k8s_client . V1SecretKeySelector ( name = GITLAB_CREDENTIALS_STORE , key = GITLAB_DEPLOY_TOKEN_KEY ) ) ) )","title":"Mounting Kubernetes Secrets to Environment Variables in Container Operations"},{"location":"3-Pipelines/Kubeflow-Pipelines/#parameterizing-pipelines","text":"Whenever possible, create pipelines in a generic way: define parameters that might change as pipeline inputs instead of writing values directly in your Python code. For example, if you want a pipeline to process data from minimal-tenant/john-smith/data1.csv , don't hard code that path - instead, accept it as a pipeline parameter. This way you can call the same pipeline repeatedly by passing it the data location as an argument. You can see this approach in our example notebooks , where we accept MinIO credentials and the location to store our results as pipeline parameters.","title":"Parameterizing pipelines"},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-complexlarge-data-tofrom-a-pipeline","text":"Although small data can often be stringified, passing by string is not suitable for complex data (large parquet files, images, etc.). It is common to use blob storage (for example: MinIO ) or other outside storage methods to persist data between components or even for later use. A typical pattern would be: Upload large/complex input data to blob storage (e.g. training data, a saved model, etc.) Pass the location of this data into the pipeline as parameters, and make your pipeline/components fetch the data as required For each component in a pipeline, specify where they place outputs in the same way For each component also return the path where it has stored its data (in this case, the string we passed it in the above bullet). This feels redundant, but it is a common pattern that lets you chain operations together Here is a schematic example of this pattern: def my_blobby_pipeline ( path_to_numbers_1 , path_to_numbers_2 , path_for_output ): \"\"\" Averaging pipeline which accepts two groups of numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op_that_takes_path_to_blob ( path_to_numbers = path_to_numbers_1 , output_location = path_for_output + \"/avg_1\" ) avg_2 = average_op_that_takes_path_to_blob ( numbers = path_to_numbers_2 , output_location = path_for_output + \"/avg_2\" ) # Note that this assumes the average_op can take multiple paths to numbers. You could also have an # aggregation component that combines avg_1 and avg_2 into a single file of numbers paths_to_numbers = [ avg_1 . output , avg_2 . output ] average_result_overall = average_op ( path_to_numbers = paths_to_numbers , output_location = path_for_output + \"/average_result_overall\" ) Within this platform, the primary method for persisting large files is through MinIO as described in our Storage documentation . Examples of this are also described in our example notebooks (also found in jupyter-notebooks/self-serve-storage/ on any notebook server).","title":"Passing complex/large data to/from a pipeline"},{"location":"3-Pipelines/Kubeflow-Pipelines/#typical-development-patterns","text":"","title":"Typical development patterns"},{"location":"3-Pipelines/Kubeflow-Pipelines/#end-to-end-pipeline-development","text":"A typical pattern for building pipelines in Kubeflow Pipelines is: Define components for each of your tasks Compose your components in a @dsl.pipeline decorated function compile() your pipeline, upload your YAML files, and run This pattern lets you define portable components that can be individually tested before combining them into a full pipeline. Depending on the type and complexity of task, there are different methods for building the components.","title":"End-to-end pipeline development"},{"location":"3-Pipelines/Kubeflow-Pipelines/#methods-for-authoring-components","text":"Fundamentally, every component in Kubeflow Pipelines runs a container. Kubeflow Pipelines offers several methods to define these components with different levels of flexibility and complexity.","title":"Methods for authoring components"},{"location":"3-Pipelines/Kubeflow-Pipelines/#user-defined-container-components","text":"You can define tasks through custom Docker images. The design pattern for this is: Define (update) code for your task and commit to Git Build an image from your task (through manual command or CI pipeline) Test running this Docker image locally (and iterate if needed) Push the image to a container registry (usually Docker hub, but it will be Azure Container Registry in our case on the Advanced Analytics Workspace) Update the Kubeflow Pipeline to point to the new image (via dsl.ContainerOp like above) and test the pipeline This lets you run anything you can put into a Docker image as a task in Kubeflow Pipelines. You can manage and test your images and have complete control over how they run and what dependencies use. The docker run interface for each container becomes the API that Kubeflow Pipelines dsl.ContainerOp interacts with \u2013 running the containers is effectively like running them locally using a terminal. Anything you can make into a container with that interface can be run in Kubeflow Pipelines. !!! danger \"...however, for security reasons the platform currently does not allow users to build/run custom Docker images. This is planned for the future, but in interim see Lightweight components for a way to develop pipelines without custom images\"","title":"User-defined container components"},{"location":"3-Pipelines/Kubeflow-Pipelines/#lightweight-python-components","text":"While full custom containers offer great flexibility, sometimes they're heavier than needed. The Kubeflow Pipelines SDK also allows for Lightweight Python Components , which are components that can be built straight from Python without building new container images for each change. These components are great for fast iteration during development, as well as for simple tasks that can be written and managed easily. This is an example of a lightweight pipeline with a single component that concatenates strings: import kfp from kfp import dsl from kfp.components import func_to_container_op def concat_string ( a , b ) -> str : return f \"( { a } | { b } )\" concat_string_component = func_to_container_op ( concat_string , base_image = \"python:3.8.3-buster\" ) @dsl . pipeline ( name = \"My lightweight pipeline\" , ) def pipeline ( str1 , str2 , str3 ): # Note that we use the concat_string_component, not the # original concat_string() function concat_result_1 = concat_string_component ( str1 , str2 ) # By using cancat_result_1's output, we define the dependency of # concat_result_2 on concat_result_1 concat_result_2 = concat_string_component ( concat_result_1 . output , str3 ) We see that our concat_string component is defined directly in Python rather than from a Docker image. In the end, our function still runs in a container, but we don't have to built it ourselves: func_to_container_op() runs our Python code inside the provided base image ( python:3.8.3-buster ). This lets use avoid building every time we change our code. The base image can be anything accessible by Kubeflow, which includes all images in the Azure Container Registry and any whitelisted images from Docker hub. Lightweight components have a number of advantages but also some drawbacks See this description of their basic characteristics, as well as this example which uses them in a more complex pipeline A convenient base image to use is the the image your notebook server is running By using the same image as your notebook server, you ensure Kubeflow Pipelines has the same packages available to it as the notebook where you do your analysis. This can help avoid errors from importing packages specific to your environment. You can find that link from the notebook server page as shown below, but make sure you prepend the registry URL (so the below image would have base_image=k8scc01covidacr.azurecr.io/machine-learning-notebook-cpu:562fa4a2899eeb9ae345c51c2491447ec31a87d7 ). Note that while using a fully featured base image for iteration is fine, it's good practice to keep production pipelines lean and only supply the necessary software. That way you reduce the startup time for each step in your pipeline.","title":"Lightweight Python components"},{"location":"3-Pipelines/Kubeflow-Pipelines/#defining-components-directly-in-yaml","text":"Components can be defined directly with a YAML file, where the designer can run terminal commands from a given Docker image. This can be a great way to make non-Python pipeline components from existing containers. As with all components, we can pass both arguments and data files into/out of the component. For example: name : Concat Strings inputs : - { name : Input text 1 , type : String , description : \"Some text to echo into a terminal and tee to a file\" , } - { name : Input text 2 , type : String , description : \"Some text to echo into a terminal and tee to a file\" , } outputs : - { name : Output filename , type : String } implementation : container : image : bash:5 command : - bash - -ex - -c - | echo \"$0 | $1\" | tee $2 - { inputValue : Input text 1 } - { inputValue : Input text 2 } - { outputPath : Output filename } This example concatenates two strings like our lightweight example above. We then define a component in python from this YAML: from kfp.components import load_component_from_file echo_and_tee = load_component_from_file ( 'path/to/echo_and_tee.yaml' ) @dsl . pipeline def my_pipeline (): echo_and_tee_task_1 = echo_and_tee ( \"My text to echo\" ) # A second use that consumes the return of the first one echo_and_tee_task_2 = echo_and_tee ( echo_and_tee_task_1 . output ) See this example for more details on using existing components.","title":"Defining components directly in YAML"},{"location":"3-Pipelines/Kubeflow-Pipelines/#reusing-existing-components","text":"Similar to well abstracted functions, well abstracted components can reduce the amount of code you have to write for any given project. For example, rather than teaching your machine learning train_model component to also save the resulting model to MinIO, you can instead have train_model return the model and then Kubeflow Pipelines can pass the model to a reusable copy_to_minio component. This reuse pattern applies to components defined through any means (containers, lightweight, or YAML). Take a look at our example notebook , which reuses provided components for simple file IO tasks.","title":"Reusing existing components"},{"location":"3-Pipelines/PaaS/","text":"Integrate with Platforms like Databricks and AzureML \u00b6 The AAW platform is built around the idea of integrations, and so we can integrate with many Platform as a Service (PaaS) offerings, such as Azure ML and Databricks . See some examples on our \"MLOps\" github Repo .","title":"PaaS Integration"},{"location":"3-Pipelines/PaaS/#integrate-with-platforms-like-databricks-and-azureml","text":"The AAW platform is built around the idea of integrations, and so we can integrate with many Platform as a Service (PaaS) offerings, such as Azure ML and Databricks . See some examples on our \"MLOps\" github Repo .","title":"Integrate with Platforms like Databricks and AzureML"},{"location":"3-Pipelines/Serving/","text":"Model Serving with Seldon Core and KFServing \u00b6 \u2692 This page is under construction \u2692 The person writing this entry does not know enough about this feature to write about it, but you can ask on our Slack channel. Serverless with KNative \u00b6 Kubernetes and KNative let your services scale up and down on demand. This lets you create APIs to serve Machine Learning models, without the need to manage load balancing or scale-up. The platform can handle all of your scaling for you, so that you can focus on the program logic. \u2692 This page is under construction \u2692 The person writing this entry does not know enough about this feature to write about it, but you can ask on our Slack channel.","title":"Model Serving"},{"location":"3-Pipelines/Serving/#model-serving-with-seldon-core-and-kfserving","text":"\u2692 This page is under construction \u2692 The person writing this entry does not know enough about this feature to write about it, but you can ask on our Slack channel.","title":"Model Serving with Seldon Core and KFServing"},{"location":"3-Pipelines/Serving/#serverless-with-knative","text":"Kubernetes and KNative let your services scale up and down on demand. This lets you create APIs to serve Machine Learning models, without the need to manage load balancing or scale-up. The platform can handle all of your scaling for you, so that you can focus on the program logic. \u2692 This page is under construction \u2692 The person writing this entry does not know enough about this feature to write about it, but you can ask on our Slack channel.","title":"Serverless with KNative"}]}